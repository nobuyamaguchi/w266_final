{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-for-tf2\n",
      "  Downloading bert-for-tf2-0.14.4.tar.gz (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 1.7 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting py-params>=0.9.6\n",
      "  Downloading py-params-0.9.7.tar.gz (6.8 kB)\n",
      "Collecting params-flow>=0.8.0\n",
      "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
      "Requirement already satisfied: numpy in /home/nobu_yamaguchi/anaconda3/lib/python3.7/site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.1)\n",
      "Requirement already satisfied: tqdm in /home/nobu_yamaguchi/anaconda3/lib/python3.7/site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.42.1)\n",
      "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
      "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.4-py3-none-any.whl size=30114 sha256=8add6b6b4598d4c300f4814767ee1440b896ccacbd5ae05a428af27aa710a1a0\n",
      "  Stored in directory: /home/nobu_yamaguchi/.cache/pip/wheels/6c/c9/9c/363182ea34a736dae336eeaf0dd4a7eec3c6a5afe32373e1fe\n",
      "  Building wheel for py-params (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py-params: filename=py_params-0.9.7-py3-none-any.whl size=7302 sha256=8ac0dcbc968ce8599110cba6d6bb23899f4465fb7eea4584186b35556c449a0a\n",
      "  Stored in directory: /home/nobu_yamaguchi/.cache/pip/wheels/47/3d/2d/bbffcfd6b9f4b8b5cbf07e7520ac2676192fe9431240c13ee8\n",
      "  Building wheel for params-flow (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19473 sha256=db82dd4d94cafc56d4ee6e075c4dc30fc1c29e662d0b70567d39fb5248e18166\n",
      "  Stored in directory: /home/nobu_yamaguchi/.cache/pip/wheels/0e/fc/d2/a44fff33af0f233d7def6e7de413006d57c10e10ad736fe8f5\n",
      "Successfully built bert-for-tf2 py-params params-flow\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
      "Successfully installed bert-for-tf2-0.14.4 params-flow-0.8.2 py-params-0.9.7\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/nobu_yamaguchi/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.91\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/nobu_yamaguchi/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.8.0-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 2.7 MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /home/nobu_yamaguchi/anaconda3/lib/python3.7/site-packages (from tensorflow_hub) (3.12.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/nobu_yamaguchi/anaconda3/lib/python3.7/site-packages (from tensorflow_hub) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/nobu_yamaguchi/anaconda3/lib/python3.7/site-packages (from tensorflow_hub) (1.18.1)\n",
      "Requirement already satisfied: setuptools in /home/nobu_yamaguchi/anaconda3/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow_hub) (45.2.0.post20200210)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.8.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/nobu_yamaguchi/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import numpy as up\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 31)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patent_abstract = pd.read_csv('patent_abstract_5000.csv')\n",
    "patent_abstract.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEHCAYAAABfkmooAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAVe0lEQVR4nO3de7DkZX3n8fdHxgt4A2RgqYFkMJkymiy3TJAs8QYGBoyAW5rCcuPEYjN7wYru5iJkd0PipUprd8VY2ZCQwAqoAcQLE3SDI0I0uyswXOQqOxMgMjusMzooqBEDfvePfo424znz9Jk5fU4fzvtV1dW/3/N7ft3fB3rmM79LP52qQpKkXXnaQhcgSZp8hoUkqcuwkCR1GRaSpC7DQpLUtWyhCxiHAw44oFauXLnQZUjSonLzzTd/vaqWT7ftKRkWK1euZOPGjQtdhiQtKkn+fqZtnoaSJHUZFpKkLsNCktRlWEiSusYaFkkeSHJHktuSbGxt+yfZkGRTe96vtSfJB5NsTnJ7kqOHXmdt678pydpx1ixJ+nHzcWTxqqo6sqpWt/WzgWurahVwbVsHOBlY1R7rgPNhEC7AucBLgWOAc6cCRpI0PxbiNNRpwMVt+WLg9KH2S2rgS8C+SQ4GTgI2VNWOqnoY2ACsme+iJWkpG3dYFPDZJDcnWdfaDqqqhwDa84GtfQXw4NC+W1rbTO1PkmRdko1JNm7fvn2OhyFJS9u4v5R3XFVtTXIgsCHJV3bRN9O01S7an9xQdQFwAcDq1av9kQ5JmkNjDYuq2tqetyX5JINrDl9LcnBVPdROM21r3bcAhw7tfgiwtbW/cqf268dZ98qzPz3Ol5/RA+99zYK8ryT1jO00VJJnJ3nu1DJwInAnsB6YuqNpLXBVW14PvLndFXUs8K12muoa4MQk+7UL2ye2NknSPBnnkcVBwCeTTL3PR6vqr5PcBFyR5Ezgq8AbWv/PAKcAm4HvAm8BqKodSd4F3NT6vbOqdoyxbknSTsYWFlV1H3DENO3fAE6Ypr2As2Z4rYuAi+a6RknSaPwGtySpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldYw+LJHsluTXJ1W39sCQ3JNmU5PIkz2jtz2zrm9v2lUOvcU5rvzfJSeOuWZL0ZPNxZPE24J6h9fcB51XVKuBh4MzWfibwcFX9NHBe60eSlwBnAD8LrAH+JMle81C3JKkZa1gkOQR4DfAXbT3A8cCVrcvFwOlt+bS2Ttt+Qut/GnBZVT1WVfcDm4Fjxlm3JOnJxn1k8QHgd4EftPUXAN+sqsfb+hZgRVteATwI0LZ/q/X/Yfs0+0iS5sHYwiLJrwDbqurm4eZpulZn2672GX6/dUk2Jtm4ffv2WdcrSZrZOI8sjgNOTfIAcBmD008fAPZNsqz1OQTY2pa3AIcCtO3PB3YMt0+zzw9V1QVVtbqqVi9fvnzuRyNJS9jYwqKqzqmqQ6pqJYML1J+vqjcB1wGvb93WAle15fVtnbb981VVrf2MdrfUYcAq4MZx1S1J+nHL+l3m3DuAy5K8G7gVuLC1XwhcmmQzgyOKMwCq6q4kVwB3A48DZ1XVE/NftiQtXfMSFlV1PXB9W76Pae5mqqrvAW+YYf/3AO8ZX4WSpF3xG9ySpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoaKSyS/Ny4C5EkTa5Rjyz+NMmNSf5tkn3HWpEkaeKMFBZV9UvAm4BDgY1JPprkl8damSRpYox8zaKqNgH/EXgH8Argg0m+kuSfj6s4SdJkGPWaxeFJzgPuAY4HXltVL27L582wz7PaqasvJ7kryR+29sOS3JBkU5LLkzyjtT+zrW9u21cOvdY5rf3eJCft0YglSbM26pHFHwO3AEdU1VlVdQtAVW1lcLQxnceA46vqCOBIYE2SY4H3AedV1SrgYeDM1v9M4OGq+mkGAfQ+gCQvAc4AfhZYA/xJkr1mN0xJ0p4YNSxOAT5aVf8AkORpSfYBqKpLp9uhBr7dVp/eHsXgaOTK1n4xcHpbPq2t07afkCSt/bKqeqyq7gc2A8eMWLckaQ6MGhafA/YeWt+nte1Skr2S3AZsAzYAfwd8s6oeb122ACva8grgQYC2/VvAC4bbp9ln+L3WJdmYZOP27dtHHJYkaRSjhsWzho4SaMv79Haqqieq6kjgEAZHAy+erlt7zgzbZmrf+b0uqKrVVbV6+fLlvdIkSbMwalh8J8nRUytJfh74h1HfpKq+CVwPHAvsm2RZ23QIsLUtb2Fway5t+/OBHcPt0+wjSZoHo4bF24GPJfliki8ClwNv3dUOSZZPfYEvyd7AqxncTXUd8PrWbS1wVVte39Zp2z9fVdXaz2h3Sx0GrAJuHLFuSdIcWNbvAlV1U5KfAV7E4LTQV6rqHzu7HQxc3O5cehpwRVVdneRu4LIk7wZuBS5s/S8ELk2ymcERxRntve9KcgVwN/A4cFZVPTGrUUqS9shIYdH8ArCy7XNUEqrqkpk6V9XtwFHTtN/HNHczVdX3gDfM8FrvAd4zi1olSXNopLBIcinwU8BtwNS/6guYMSwkSU8dox5ZrAZe0q4hSJKWmFEvcN8J/JNxFiJJmlyjHlkcANyd5EYG03gAUFWnjqUqSdJEGTUs/mCcRUiSJtuot87+TZKfBFZV1efavFBO5idJS8SoU5T/BoPJ/f6sNa0APjWuoiRJk2XUC9xnAccBj8APfwjpwHEVJUmaLKOGxWNV9f2plTZ3k7fRStISMWpY/E2S3wP2br+9/THgr8ZXliRpkowaFmcD24E7gH8FfIaZfyFPkvQUM+rdUD8A/rw9JElLzKhzQ93P9D849MI5r0iSNHFmMzfUlGcxmB12/7kvR5I0iUa6ZlFV3xh6/N+q+gBw/JhrkyRNiFFPQx09tPo0Bkcazx1LRZKkiTPqaaj/OrT8OPAA8KtzXo0kaSKNejfUq8ZdiCRpco16Gurf72p7Vb1/bsqRJE2i2dwN9QvA+rb+WuALwIPjKEqSNFlm8+NHR1fVowBJ/gD4WFX9y3EVJkmaHKNO9/ETwPeH1r8PrJzzaiRJE2nUI4tLgRuTfJLBN7lfB1wytqokSRNl1Luh3pPkfwAva01vqapbx1eWJGmSjHoaCmAf4JGq+iNgS5LDxlSTJGnCjPqzqucC7wDOaU1PBz48rqIkSZNl1COL1wGnAt8BqKqtON2HJC0Zo4bF96uqaNOUJ3n2+EqSJE2aUcPiiiR/Buyb5DeAz+EPIUnSkjHq3VD/pf329iPAi4Dfr6oNY61MkjQxumGRZC/gmqp6NWBASNIS1D0NVVVPAN9N8vx5qEeSNIFG/Qb394A7kmyg3REFUFW/OZaqJEkTZdQL3J8G/hODmWZvHnrMKMmhSa5Lck+Su5K8rbXvn2RDkk3teb/WniQfTLI5ye3Dv86XZG3rvynJ2t0ZqCRp9+3yyCLJT1TVV6vq4t147ceB36qqW5I8F7i5HZn8OnBtVb03ydnA2Qy+8HcysKo9XgqcD7w0yf7AuQymSa/2Ouur6uHdqEmStBt6RxafmlpI8vHZvHBVPVRVt7TlR4F7gBXAacBU+FwMnN6WTwMuqYEvMbhN92DgJGBDVe1oAbEBWDObWiRJe6YXFhlafuHuvkmSlcBRwA3AQVX1EAwCBTiwdVvBk39MaUtrm6l95/dYl2Rjko3bt2/f3VIlSdPohUXNsDyyJM8BPg68vaoe2VXXGd5/pvYnN1RdUFWrq2r18uXLd6dUSdIMemFxRJJHkjwKHN6WH0nyaJJd/cUPQJKnMwiKj1TVJ1rz19rpJdrztta+BTh0aPdDgK27aJckzZNdhkVV7VVVz6uq51bVsrY8tf68Xe2bJMCFwD1V9f6hTeuBqTua1gJXDbW/ud0VdSzwrXaa6hrgxCT7tTunTmxtkqR5Mur3LHbHccCvMfh+xm2t7feA9zKYa+pM4KvAG9q2zwCnAJuB7wJvAaiqHUneBdzU+r2zqnaMsW5J0k7GFhZV9bdMf70B4IRp+hdw1gyvdRFw0dxVJ0majdn8Up4kaYkyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqGltYJLkoybYkdw617Z9kQ5JN7Xm/1p4kH0yyOcntSY4e2mdt678pydpx1StJmtk4jyw+BKzZqe1s4NqqWgVc29YBTgZWtcc64HwYhAtwLvBS4Bjg3KmAkSTNn7GFRVV9AdixU/NpwMVt+WLg9KH2S2rgS8C+SQ4GTgI2VNWOqnoY2MCPB5Akaczm+5rFQVX1EEB7PrC1rwAeHOq3pbXN1P5jkqxLsjHJxu3bt8954ZK0lE3KBe5M01a7aP/xxqoLqmp1Va1evnz5nBYnSUvdfIfF19rpJdrztta+BTh0qN8hwNZdtEuS5tF8h8V6YOqOprXAVUPtb253RR0LfKudproGODHJfu3C9omtTZI0j5aN64WT/CXwSuCAJFsY3NX0XuCKJGcCXwXe0Lp/BjgF2Ax8F3gLQFXtSPIu4KbW751VtfNFc0nSmI0tLKrqjTNsOmGavgWcNcPrXARcNIelSZJmaVIucEuSJphhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVLXogmLJGuS3Jtkc5KzF7oeSVpKli10AaNIshfw34BfBrYANyVZX1V3L2xlc2vl2Z9ekPd94L2vWZD3lbR4LJYji2OAzVV1X1V9H7gMOG2Ba5KkJWNRHFkAK4AHh9a3AC8d7pBkHbCurX47yb178H4HAF/fg/0nwchjyPvGXMnuW1L/HyaYY5gM8zGGn5xpw2IJi0zTVk9aqboAuGBO3izZWFWr5+K1FopjmAyOYTI4hj23WE5DbQEOHVo/BNi6QLVI0pKzWMLiJmBVksOSPAM4A1i/wDVJ0pKxKE5DVdXjSd4KXAPsBVxUVXeN8S3n5HTWAnMMk8ExTAbHsIdSVf1ekqQlbbGchpIkLSDDQpLUZVgMWYxTiiS5KMm2JHcOte2fZEOSTe15v4WssSfJoUmuS3JPkruSvK21L5pxJHlWkhuTfLmN4Q9b+2FJbmhjuLzdoDHRkuyV5NYkV7f1RTWGJA8kuSPJbUk2trZF81makmTfJFcm+Ur7s/GLCzkOw6IZmlLkZOAlwBuTvGRhqxrJh4A1O7WdDVxbVauAa9v6JHsc+K2qejFwLHBW+2+/mMbxGHB8VR0BHAmsSXIs8D7gvDaGh4EzF7DGUb0NuGdofTGO4VVVdeTQ9xIW02dpyh8Bf11VPwMcweD/ycKNo6p8DC7y/yJwzdD6OcA5C13XiLWvBO4cWr8XOLgtHwzcu9A1znI8VzGYB2xRjgPYB7iFwSwDXweWtfYnfcYm8cHgO0zXAscDVzP4QuxiG8MDwAE7tS2qzxLwPOB+2k1IkzAOjyx+ZLopRVYsUC176qCqegigPR+4wPWMLMlK4CjgBhbZONrpm9uAbcAG4O+Ab1bV463LYvhMfQD4XeAHbf0FLL4xFPDZJDe3aYBgkX2WgBcC24H/3k4J/kWSZ7OA4zAsfqQ7pYjGK8lzgI8Db6+qRxa6ntmqqieq6kgG/zo/BnjxdN3mt6rRJfkVYFtV3TzcPE3XiR1Dc1xVHc3glPJZSV6+0AXthmXA0cD5VXUU8B0W+NSZYfEjT6UpRb6W5GCA9rxtgevpSvJ0BkHxkar6RGtedOMAqKpvAtczuP6yb5KpL79O+mfqOODUJA8wmNn5eAZHGotpDFTV1va8Dfgkg+BebJ+lLcCWqrqhrV/JIDwWbByGxY88laYUWQ+sbctrGVwDmFhJAlwI3FNV7x/atGjGkWR5kn3b8t7AqxlckLwOeH3rNtFjqKpzquqQqlrJ4PP/+ap6E4toDEmeneS5U8vAicCdLKLPEkBV/T/gwSQvak0nAHezkONY6As5k/QATgH+D4Nzzf9hoesZsea/BB4C/pHBv0bOZHCe+VpgU3vef6Hr7Izhlxic2rgduK09TllM4wAOB25tY7gT+P3W/kLgRmAz8DHgmQtd64jjeSVw9WIbQ6v1y+1x19Sf48X0WRoay5HAxvaZ+hSw30KOw+k+JEldnoaSJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIspGkk+c02LfRHZrnf9UlW93s+aZ9Tp6bET3J6b7bjJL/Tpt++LcmdSZ5Isv9s3lOaLb9nIU0jyVeAk6vq/lnudz3w21W1cTff90MMvgx35Yj9Xwv8u6o6fnfeTxqVRxbSTpL8KYNvAq9P8mj7EZok+UaSN7c+lyZ5dZK9k1yW5PYklwN7d157TZJb2o8kXdvafj3JHyf5Z8CpwH9uRw0/1Y5w7m6vf9k0L/lGBt/il8ZqWb+LtLRU1b9OsgZ4FfBuBhPs/T1wH/Ay4BIGkwT+m/b4blUdnuRwBr9jMa0ky4E/B15eVffvfOqoqv5XkvUMHVm001OHVdVjU3NPDb3ePgx++OqtczFuaVc8spB27YvAy9vjfOCfJlkB7Kiqb7f2DwNU1e0M5vGZybHAF6ZObVXVjhHe/3bgI0n+BYNfFBz2WuB/jvg60h4xLKRd+wKDo4mXMZh2fDuDGVi/ONRn1At/mUXfKa9h8HO/Pw/cPDRVOAxmhvUUlOaFYSHtQlU9CBwArKqq+4C/BX6bH4XFF4A3AST5OQazz87kfwOvSHJY6z/dHUyPAlNTbD8NOLSqrmPw63X7As9p254PvIIJn2pbTx2GhdR3A4Op62EQEisYhAYMTk09J8ntDP5Cv3GmF6mq7cA64BNJvgxcPk23y4DfSXIrsAr4cJI7GEx/fl4NflgJ4HXAZ6vqO3s0MmlE3jorSeryyEKS1OWts9IYJLkBeOZOzb9WVXcsRD3SnvI0lCSpy9NQkqQuw0KS1GVYSJK6DAtJUtf/B0b3QIgCRfXzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(patent_abstract['fwd_cits7'], density=False)  # `density=False` would make counts\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('fwd_cits7');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "\n",
    "    (patent_abstract['fwd_cits7'] < 1),\n",
    "    (patent_abstract['fwd_cits7'] >= 1)\n",
    "]\n",
    "\n",
    "choices = ['0', '1']\n",
    "\n",
    "patent_abstract['fwd_cits7_rank'] = np.select(condlist=conditions, choicelist=choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = patent_abstract[['text', 'fwd_cits7_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3408\n",
       "1    1592\n",
       "Name: fwd_cits7_rank, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patent_abstract['fwd_cits7_rank'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "sentences = list(df['text'])\n",
    "for sen in sentences:\n",
    "    abstracts.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text' 'fwd_cits7_rank']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fwd_cits7_rank.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.fwd_cits7_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_abstracts(text_abstracts):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_abstracts = [tokenize_abstracts(abstract) for abstract in abstracts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerparing Data For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_with_len = [[abstract, y[i], len(abstract)]\n",
    "                 for i, abstract in enumerate(tokenized_abstracts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(abstracts_with_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_with_len.sort(key=lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_abstracts_labels = [(abstract_lab[0], abstract_lab[1]) for abstract_lab in abstracts_with_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_abstracts_labels, output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 23), dtype=int32, numpy=\n",
       " array([[ 1037,  8822,  2291,  2005,  2019,  2948,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [ 4725,  2005,  8225,  1996,  2206,  7328,  2024, 21362,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [ 1996, 11028, 14623,  2000,  2019, 14794,  9179, 17564,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [ 1996, 11028, 14623,  2000, 25416, 22648,  7062, 14692,  8670,\n",
       "         18142,  8962,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [ 4725,  1998,  9265,  3141,  2000, 14126,  6074,  2000, 13656,\n",
       "          8153,  2024,  2649,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [ 1996, 11028, 14623,  2000, 14692, 11307,  1998,  4118,  2005,\n",
       "          4526, 14692, 11307,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [ 1037,  2832,  2005,  1996,  7547,  1997,  1996,  7328,  1997,\n",
       "          5675,  2003,  2649,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [ 1996,  2556, 11028,  3640, 10099,  9265, 21739,  1998,  4725,\n",
       "          1997,  2478,  1996,  2168,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [ 8331, 15110,  2164, 19706,  5412,  2000,  2022,  8546,  3442,\n",
       "          2240,  2005,  7176,  1996, 19706,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [21362,  2182,  2378,  2024,  6194,  2005,  5716,  6519, 19419,\n",
       "          5648, 16942,  3421,  2011,  5675,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [ 1996,  2556, 11028, 14623,  2000,  4118,  2005, 17739, 17824,\n",
       "          1051, 10085, 17250,  2030, 17824, 28086,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [ 1996, 11028, 14623,  2000,  4725,  2005,  5155,  2632, 25383,\n",
       "         18124,  2015,  1998,  1996, 19577,  1998,  7312,  3688, 21739,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [ 1996,  2556, 11028, 14623,  2000, 19782,  2828,  4318,  8808,\n",
       "          2030, 11825,  2833,  4031,  1998,  2000,  2049,  5814,  2832,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [ 1037,  2606,  2729,  5512,  9605,  2013,  1059,  2102,  2000,\n",
       "          1059,  2102,  1997,  1996,  2561,  5512,  1997, 23431,  5474,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [ 9265,  1998,  4725,  2005,  1996, 11616,  3949,  1998,  9740,\n",
       "          1997,  9253, 24759, 15396,  1043,  3609, 22471,  2389,  4456,\n",
       "             0,     0,     0,     0,     0],\n",
       "        [ 1996, 11028,  3640, 22931,  2114, 10882, 12618, 28522,  3367,\n",
       "         13791,  5250,  6904,  2361,  1998,  4725,  1997,  2478,  1996,\n",
       "          2168,     0,     0,     0,     0],\n",
       "        [ 1996, 11028, 14623,  2000,  5080,  2005,  3806, 10808,  1998,\n",
       "         14178,  9031,  2005,  5001, 10464,  9363,  7685,  2478,  8060,\n",
       "         15946,     0,     0,     0,     0],\n",
       "        [ 1996,  2556, 11028, 14623,  2000, 20377,  5051,  6895,  8873,\n",
       "          2278, 22931,  2114,  4647,  2863,  1998,  3729,  2037,  9922,\n",
       "          1998,  2224,     0,     0,     0],\n",
       "        [ 1996,  2556, 11028, 14623,  2000,  2047,  3424, 11263, 13807,\n",
       "          2140,  9265,  1998,  2037,  2224,  1999,  1996,  3949,  1997,\n",
       "          4910,  3688,     0,     0,     0],\n",
       "        [ 1037,  6302,  6767, 24458,  2594,  5080,  2064,  2421, 20681,\n",
       "         16888,  2121,  6741,  2007,  5301, 28353, 27759,  2425,  9496,\n",
       "          3207, 10296,     0,     0,     0],\n",
       "        [ 1996,  7107,  4646, 14623,  2000, 22822, 21850,  7229, 16942,\n",
       "          1997,  5675,  2007,  8760, 12353,  1999, 12318,  4319, 22423,\n",
       "          1998, 26641,     0,     0,     0],\n",
       "        [ 1037, 15026,  3720,  2383, 16305, 15026,  2007,  6741,  1997,\n",
       "         21396, 14876, 12556,  8360, 18898,  1998,  4725,  1997,  2437,\n",
       "          1998,  2478,     0,     0,     0],\n",
       "        [ 1996, 11028,  3640,  3424,  4429, 25032, 22931,  1998, 10047,\n",
       "         23041, 24163,  2078,  9103,  5867,  2015,  1998,  4725,  1997,\n",
       "          2478,  1996,  2168,     0,     0],\n",
       "        [ 1037,  4226,  3560,  7300,  1997,  1996,  8915,  6494, 11368,\n",
       "         29598,  3286, 26387,  5474,  1997, 17770, 14573,  2401,  6844,\n",
       "         27165,  6540,  2917,     0,     0],\n",
       "        [ 1996,  2556, 11028, 14623,  2000,  1996,  2492,  1997,  2966,\n",
       "         16474,  2015,  1998,  2062,  4919,  2000,  8720,  1997,  2572,\n",
       "         27678,  4588,  8331,     0,     0],\n",
       "        [ 1037,  4118,  1998, 14709,  2005, 11737,  6562, 11320, 22311,\n",
       "          2854,  2005,  7497,  1998,  2951,  6726,  1999,  5710,  2422,\n",
       "          4806,  1058, 15472,     0,     0],\n",
       "        [ 1996,  2556, 11028, 14623,  2000, 17492, 16031,  7888,  2164,\n",
       "          3526, 18845,  3366, 16662,  2593,  2894,  2030,  1999,  5257,\n",
       "          2007,  2060,  6039,  2545,     0],\n",
       "        [ 1996, 11028, 14623,  2000,  2019,  1041, 20614,  2953,  1999,\n",
       "          3327,  2005,  2346, 22491,  3698,  2007,  2029,  1996,  6032,\n",
       "          3947,  2003,  9839, 11038,     0],\n",
       "        [ 7861,  5092, 21341,  2015,  1997,  1996,  2556, 11028,  3444,\n",
       "          3117,  7987,  7677,  3593,  9265,  4725,  1997,  2437,  1998,\n",
       "          4725,  1997, 12318,  4295,     0],\n",
       "        [ 3117, 21864,  3630,  4179, 10099,  1997,  5675, 13859,  9265,\n",
       "          4820,  2107, 10099,  1998,  2037,  2224,  1999,  7242,  2004,\n",
       "         22953,  5302,  9527,  8113, 25456],\n",
       "        [ 2122, 11320, 11233, 27654,  6351,  9309,  2031,  2540,  2081,\n",
       "          1997,  6351, 17782,  2594,  6058, 18898,  4820,  2012,  2560,\n",
       "          2028, 11320, 10020,  7361, 16892],\n",
       "        [ 1996, 11028, 14623,  2000, 12509,  8021,  2030, 12509, 19806,\n",
       "          4118,  2005,  5155,  3569, 12379, 28688,  1998,  2000,  4118,\n",
       "          2005,  5814, 22157, 10163,  6046]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 1], dtype=int32)>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(batched_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_BATCHES = np.math.ceil(len(sorted_abstracts_labels) / BATCH_SIZE)\n",
    "TEST_BATCHES = TOTAL_BATCHES // 20\n",
    "batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "test_data = batched_dataset.take(TEST_BATCHES)\n",
    "train_data = batched_dataset.skip(TEST_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3) \n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                        embedding_dimensions=EMB_DIM,\n",
    "                        cnn_filters=CNN_FILTERS,\n",
    "                        dnn_units=DNN_UNITS,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_CLASSES == 2:\n",
    "    text_model.compile(loss=\"binary_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"acc\"])\n",
    "else:\n",
    "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "150/150 [==============================] - 27s 178ms/step - loss: 0.6326 - acc: 0.6809\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 26s 174ms/step - loss: 0.5907 - acc: 0.6876\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 26s 173ms/step - loss: 0.3540 - acc: 0.8474\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 26s 173ms/step - loss: 0.0585 - acc: 0.9879\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 26s 171ms/step - loss: 0.0103 - acc: 0.9987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f10c2089c50>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.fit(train_data, epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7/Unknown - 0s 31ms/step - loss: 1.4242 - acc: 0.5402[1.4242296133722578, 0.5401786]\n"
     ]
    }
   ],
   "source": [
    "results = text_model.evaluate(test_data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
