{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>publication_number</th>\n",
       "      <th>application_number</th>\n",
       "      <th>text</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>new_appl_nbr</th>\n",
       "      <th>appln_id</th>\n",
       "      <th>app_nbr</th>\n",
       "      <th>filing</th>\n",
       "      <th>tech_field</th>\n",
       "      <th>...</th>\n",
       "      <th>fwd_cits7_xy</th>\n",
       "      <th>breakthrough</th>\n",
       "      <th>breakthrough_xy</th>\n",
       "      <th>generality</th>\n",
       "      <th>originality</th>\n",
       "      <th>radicalness</th>\n",
       "      <th>renewal</th>\n",
       "      <th>quality_index_4</th>\n",
       "      <th>quality_index_6</th>\n",
       "      <th>quality_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1117907</td>\n",
       "      <td>EP-3412977-A1</td>\n",
       "      <td>EP-18173805-A</td>\n",
       "      <td>A radiant module (1) comprising: a first hollo...</td>\n",
       "      <td>20180523</td>\n",
       "      <td>EP20180173805</td>\n",
       "      <td>494709760</td>\n",
       "      <td>EP20180173805</td>\n",
       "      <td>2018</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.317460</td>\n",
       "      <td>0.293740</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>836155</td>\n",
       "      <td>EP-3062394-A1</td>\n",
       "      <td>EP-15200990-A</td>\n",
       "      <td>An array antenna device of this disclosure inc...</td>\n",
       "      <td>20151218</td>\n",
       "      <td>EP20150200990</td>\n",
       "      <td>447596715</td>\n",
       "      <td>EP20150200990</td>\n",
       "      <td>2015</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.216450</td>\n",
       "      <td>0.312182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36339</td>\n",
       "      <td>EP-2591987-A1</td>\n",
       "      <td>EP-10854435-A</td>\n",
       "      <td>A vehicle hood structure capable of improving ...</td>\n",
       "      <td>20100708</td>\n",
       "      <td>EP20100854435</td>\n",
       "      <td>340657535</td>\n",
       "      <td>EP20100854435</td>\n",
       "      <td>2010</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.140097</td>\n",
       "      <td>0.243247</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536885</td>\n",
       "      <td>EP-2863948-A2</td>\n",
       "      <td>EP-13807400-A</td>\n",
       "      <td>This invention relates generally to the genera...</td>\n",
       "      <td>20130621</td>\n",
       "      <td>EP20130807400</td>\n",
       "      <td>413456628</td>\n",
       "      <td>EP20130807400</td>\n",
       "      <td>2013</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.707031</td>\n",
       "      <td>0.838169</td>\n",
       "      <td>0.518812</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.702178</td>\n",
       "      <td>0.560783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>433243</td>\n",
       "      <td>EP-2854677-A1</td>\n",
       "      <td>EP-13742496-A</td>\n",
       "      <td>Methods and implants to treat anterior cruciat...</td>\n",
       "      <td>20130530</td>\n",
       "      <td>EP20130742496</td>\n",
       "      <td>409439382</td>\n",
       "      <td>EP20130742496</td>\n",
       "      <td>2013</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.233974</td>\n",
       "      <td>0.267708</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>26522</td>\n",
       "      <td>EP-2514281-A1</td>\n",
       "      <td>EP-10838186-A</td>\n",
       "      <td>A plasma gun system comprising: a plasma gun c...</td>\n",
       "      <td>20101213</td>\n",
       "      <td>EP20100838186</td>\n",
       "      <td>334899458</td>\n",
       "      <td>EP20100838186</td>\n",
       "      <td>2010</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.853203</td>\n",
       "      <td>0.406780</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.315754</td>\n",
       "      <td>0.293119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>357146</td>\n",
       "      <td>EP-2814870-A1</td>\n",
       "      <td>EP-12813624-A</td>\n",
       "      <td>An amorphous copolyester comprising the reacti...</td>\n",
       "      <td>20121211</td>\n",
       "      <td>EP20120813624</td>\n",
       "      <td>380483196</td>\n",
       "      <td>EP20120813624</td>\n",
       "      <td>2012</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.120370</td>\n",
       "      <td>0.202268</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>810878</td>\n",
       "      <td>EP-2996404-A1</td>\n",
       "      <td>EP-15183744-A</td>\n",
       "      <td>A communication device for handling a selectio...</td>\n",
       "      <td>20150903</td>\n",
       "      <td>EP20150183744</td>\n",
       "      <td>444259224</td>\n",
       "      <td>EP20150183744</td>\n",
       "      <td>2015</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.127451</td>\n",
       "      <td>0.207662</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>374199</td>\n",
       "      <td>EP-2745969-A1</td>\n",
       "      <td>EP-12198175-A</td>\n",
       "      <td>The toll has a tool shank that is provided wit...</td>\n",
       "      <td>20121219</td>\n",
       "      <td>EP20120198175</td>\n",
       "      <td>379202539</td>\n",
       "      <td>EP20120198175</td>\n",
       "      <td>2012</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.458977</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.199134</td>\n",
       "      <td>0.322634</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>56836</td>\n",
       "      <td>EP-2406281-A2</td>\n",
       "      <td>EP-10713149-A</td>\n",
       "      <td>The present invention is directed to an isolat...</td>\n",
       "      <td>20100311</td>\n",
       "      <td>EP20100713149</td>\n",
       "      <td>314699078</td>\n",
       "      <td>EP20100713149</td>\n",
       "      <td>2010</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923571</td>\n",
       "      <td>0.396694</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.162256</td>\n",
       "      <td>0.182621</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 publication_number application_number  \\\n",
       "0        1117907      EP-3412977-A1      EP-18173805-A   \n",
       "1         836155      EP-3062394-A1      EP-15200990-A   \n",
       "2          36339      EP-2591987-A1      EP-10854435-A   \n",
       "3         536885      EP-2863948-A2      EP-13807400-A   \n",
       "4         433243      EP-2854677-A1      EP-13742496-A   \n",
       "...          ...                ...                ...   \n",
       "4995       26522      EP-2514281-A1      EP-10838186-A   \n",
       "4996      357146      EP-2814870-A1      EP-12813624-A   \n",
       "4997      810878      EP-2996404-A1      EP-15183744-A   \n",
       "4998      374199      EP-2745969-A1      EP-12198175-A   \n",
       "4999       56836      EP-2406281-A2      EP-10713149-A   \n",
       "\n",
       "                                                   text  filing_date  \\\n",
       "0     A radiant module (1) comprising: a first hollo...     20180523   \n",
       "1     An array antenna device of this disclosure inc...     20151218   \n",
       "2     A vehicle hood structure capable of improving ...     20100708   \n",
       "3     This invention relates generally to the genera...     20130621   \n",
       "4     Methods and implants to treat anterior cruciat...     20130530   \n",
       "...                                                 ...          ...   \n",
       "4995  A plasma gun system comprising: a plasma gun c...     20101213   \n",
       "4996  An amorphous copolyester comprising the reacti...     20121211   \n",
       "4997  A communication device for handling a selectio...     20150903   \n",
       "4998  The toll has a tool shank that is provided wit...     20121219   \n",
       "4999  The present invention is directed to an isolat...     20100311   \n",
       "\n",
       "       new_appl_nbr   appln_id        app_nbr  filing  tech_field  ...  \\\n",
       "0     EP20180173805  494709760  EP20180173805    2018        30.0  ...   \n",
       "1     EP20150200990  447596715  EP20150200990    2015         3.0  ...   \n",
       "2     EP20100854435  340657535  EP20100854435    2010        32.0  ...   \n",
       "3     EP20130807400  413456628  EP20130807400    2013        16.0  ...   \n",
       "4     EP20130742496  409439382  EP20130742496    2013        13.0  ...   \n",
       "...             ...        ...            ...     ...         ...  ...   \n",
       "4995  EP20100838186  334899458  EP20100838186    2010        23.0  ...   \n",
       "4996  EP20120813624  380483196  EP20120813624    2012        17.0  ...   \n",
       "4997  EP20150183744  444259224  EP20150183744    2015         4.0  ...   \n",
       "4998  EP20120198175  379202539  EP20120198175    2012        26.0  ...   \n",
       "4999  EP20100713149  314699078  EP20100713149    2010        16.0  ...   \n",
       "\n",
       "      fwd_cits7_xy  breakthrough  breakthrough_xy  generality  originality  \\\n",
       "0                0           NaN              NaN         NaN     0.777778   \n",
       "1                0           NaN              NaN         NaN     0.777778   \n",
       "2                0           NaN              NaN         NaN     0.408163   \n",
       "3                2           NaN              NaN    0.707031     0.838169   \n",
       "4                0           NaN              NaN         NaN     0.480000   \n",
       "...            ...           ...              ...         ...          ...   \n",
       "4995             0           NaN              NaN         NaN     0.853203   \n",
       "4996             0           NaN              NaN         NaN     0.625000   \n",
       "4997             0           NaN              NaN         NaN     0.625000   \n",
       "4998             0           NaN              NaN         NaN     0.458977   \n",
       "4999             1           NaN              NaN    0.000000     0.923571   \n",
       "\n",
       "      radicalness  renewal  quality_index_4  quality_index_6  quality_rank  \n",
       "0        0.916667      1.0         0.317460         0.293740             0  \n",
       "1        0.083333      3.0         0.216450         0.312182             1  \n",
       "2        0.000000      8.0         0.140097         0.243247             0  \n",
       "3        0.518812      5.0         0.702178         0.560783             1  \n",
       "4        0.400000      6.0         0.233974         0.267708             0  \n",
       "...           ...      ...              ...              ...           ...  \n",
       "4995     0.406780      9.0         0.315754         0.293119             0  \n",
       "4996     0.375000      6.0         0.120370         0.202268             0  \n",
       "4997     0.500000      3.0         0.127451         0.207662             0  \n",
       "4998     0.206897      7.0         0.199134         0.322634             1  \n",
       "4999     0.396694      9.0         0.162256         0.182621             0  \n",
       "\n",
       "[5000 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('patent_abstract_5000.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "\n",
    "    (df['quality_index_4'] < 0.3),\n",
    "    (df['quality_index_4'] >= 0.3)\n",
    "]\n",
    "\n",
    "choices = ['0', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['quality_rank'] = np.select(condlist=conditions, choicelist=choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text','quality_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lin_menghsien/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more CNN library\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500  # [Steven] I added for the CNN, to take only the 500 words.\n",
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000  # [Steven] this should be number of unique word / vocabulary\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", str(string))    \n",
    "    string = re.sub(r\"\\'\", \"\", str(string))    \n",
    "    string = re.sub(r\"\\\"\", \"\", str(string))    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = df.sample(frac=1)[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "abstracts = [] # abstracts is list of list of list to hold each sentences of each abstract (the most complete data)\n",
    "labels = [] # label is just a list holding our label which is quality_index\n",
    "texts = []  # texts to hold each complete abstract as list of list (note: abstract not breaking up to sentence level)\n",
    "for idx in range(data_train.text.shape[0]): # for each row\n",
    "    #text = BeautifulSoup(data_train.text[idx])\n",
    "    #print(clean_str(str(data_train.iloc[idx]['text'])))\n",
    "    text = clean_str(str(data_train.iloc[idx]['text'])) # text is each complete abstract\n",
    "    texts.append(text) # texts to hold each complete abstract as list of string (note: abstract not breaking up to sentence level)\n",
    "    sentences = tokenize.sent_tokenize(text) # sentences is list of string holding each complete sentence of one abstract (but it's just an intermediate variable, not used directly in later code)\n",
    "    abstracts.append(sentences) # abstracts is list of list of string to hold each sentences of each abstract (the most complete data) (this is what we use )\n",
    "    labels.append(data_train.iloc[idx]['quality_rank']) # label is just a list holding our label which is quality_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS) # intend to use next line .fit_on_texts to index each word within specific abstract at current iteration/loop, the more frequent word has lower index number, it is a dictionary format, it's like a unique vocabulary index\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "# data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17733 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  174,     7,    41, ...,     0,     0,     0],\n",
       "       [    1,    26,    40, ...,     0,     0,     0],\n",
       "       [    8,    79,     9, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [    2, 10045,   639, ...,     0,     0,     0],\n",
       "       [    1,    26,    40, ...,     0,     0,     0],\n",
       "       [    8,   104,   233, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post') # The default is pre-padding, should I try post-padding ? => padding='post', truncating='post'. I think it has to do with whether the beginning words are more important, or later words more important. You may want to try both approaches and compare result?\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 500)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (5000, 500)\n",
      "Shape of label tensor: (5000, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set\n",
      "[2632. 1368.]\n",
      "[677. 323.]\n"
     ]
    }
   ],
   "source": [
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lin_menghsien/team_repo'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import GloVe word embedding\n",
    "GLOVE_DIR = \"/home/lin_menghsien/w266/project/data/glove\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17734"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  # EMBEDDING_DIM = 100 since we import 100d GloVe\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, datetime, shutil\n",
    "from numpy.random import seed\n",
    "seed(5)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "125/125 [==============================] - 257s 2s/step - loss: 0.8257 - accuracy: 0.6062 - val_loss: 0.6348 - val_accuracy: 0.6770\n",
      "Epoch 2/15\n",
      "125/125 [==============================] - 256s 2s/step - loss: 0.6109 - accuracy: 0.6662 - val_loss: 0.6257 - val_accuracy: 0.6730\n",
      "Epoch 3/15\n",
      "125/125 [==============================] - 257s 2s/step - loss: 0.5262 - accuracy: 0.7523 - val_loss: 0.6462 - val_accuracy: 0.6690\n",
      "Epoch 4/15\n",
      "125/125 [==============================] - 258s 2s/step - loss: 0.3415 - accuracy: 0.8708 - val_loss: 0.7040 - val_accuracy: 0.6000\n",
      "Epoch 5/15\n",
      "125/125 [==============================] - 256s 2s/step - loss: 0.1513 - accuracy: 0.9550 - val_loss: 0.8078 - val_accuracy: 0.6190\n",
      "Epoch 6/15\n",
      "125/125 [==============================] - 257s 2s/step - loss: 0.1182 - accuracy: 0.9615 - val_loss: 1.2512 - val_accuracy: 0.4410\n",
      "Epoch 7/15\n",
      "125/125 [==============================] - 257s 2s/step - loss: 0.0811 - accuracy: 0.9745 - val_loss: 0.9965 - val_accuracy: 0.6260\n",
      "Epoch 8/15\n",
      "125/125 [==============================] - 258s 2s/step - loss: 0.0583 - accuracy: 0.9805 - val_loss: 1.1330 - val_accuracy: 0.6450\n",
      "Epoch 9/15\n",
      "125/125 [==============================] - 257s 2s/step - loss: 0.0355 - accuracy: 0.9887 - val_loss: 1.2518 - val_accuracy: 0.6470\n",
      "Epoch 10/15\n",
      "125/125 [==============================] - 256s 2s/step - loss: 0.0184 - accuracy: 0.9945 - val_loss: 1.4709 - val_accuracy: 0.6420\n",
      "Epoch 11/15\n",
      "125/125 [==============================] - 257s 2s/step - loss: 0.0317 - accuracy: 0.9890 - val_loss: 1.2646 - val_accuracy: 0.5650\n",
      "Epoch 12/15\n",
      "125/125 [==============================] - 257s 2s/step - loss: 0.0783 - accuracy: 0.9697 - val_loss: 1.2094 - val_accuracy: 0.5850\n",
      "Epoch 13/15\n",
      "125/125 [==============================] - 259s 2s/step - loss: 0.0447 - accuracy: 0.9837 - val_loss: 1.3533 - val_accuracy: 0.5620\n",
      "Epoch 14/15\n",
      "125/125 [==============================] - 258s 2s/step - loss: 0.0523 - accuracy: 0.9805 - val_loss: 1.4812 - val_accuracy: 0.5150\n",
      "Epoch 15/15\n",
      "125/125 [==============================] - 257s 2s/step - loss: 0.0433 - accuracy: 0.9840 - val_loss: 1.4711 - val_accuracy: 0.5870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f59d8a66bd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameter setting\n",
    "epochs = 15\n",
    "embed_dim = EMBEDDING_DIM  # already specify with EMBEDDING_DIM variable above\n",
    "dense_layer_dims = [100]\n",
    "dropout_rate = 0.2\n",
    "num_filters = [100, 100, 100]\n",
    "kernel_sizes = [48, 50, 52]\n",
    "num_classes = labels.shape[1] # \n",
    "\n",
    "# Model building\n",
    "start_time = time.time()\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "h = embedding_layer(sequence_input) # remember embedding_layer assign with GloVe's weight\n",
    "\n",
    "conv_layers_for_all_kernel_sizes = []\n",
    "counter = 0\n",
    "for kernel_size, filters in zip(kernel_sizes, num_filters): \n",
    "    conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(h)\n",
    "    conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "    counter += 1\n",
    "    #print(conv_layers_for_all_kernel_sizes)\n",
    "if counter == 1: # need this since layers.concatenate throw error when there is only 1 item in conv_layers_for_all_kernel_sizes\n",
    "    h = conv_layer\n",
    "else:\n",
    "    h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1) # Concat the feature maps from each different size.\n",
    "\n",
    "    h = keras.layers.Dropout(rate=dropout_rate)(h) # Dropout can help with overfitting (improve generalization) by randomly 0-ing different subsets of values in the vector\n",
    "\n",
    "for dim in dense_layer_dims:\n",
    "    h = keras.layers.Dense(dim, activation = 'relu')(h) # [Steven] I believe this add additional fully-connected hidden layer with the hope to increase model accuracy\n",
    "\n",
    "prediction = keras.layers.Dense(num_classes, activation='softmax')(h)\n",
    "\n",
    "model = keras.Model(inputs=sequence_input, outputs=prediction)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',  # From information theory notebooks.\n",
    "              metrics=['accuracy'])        # What metric to output as we train.\n",
    "\n",
    "# train\n",
    "model.reset_states()\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          epochs=epochs, \n",
    "          validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
