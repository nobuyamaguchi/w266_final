{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.1.0\n",
      "Hub version:  0.8.0\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bert\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import  Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow Version:\",tf.__version__)\n",
    "print(\"Hub version: \",hub.__version__)\n",
    "pd.set_option('display.max_colwidth',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_abstract = pd.read_csv('patent_abstract_5000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = patent_abstract[['text', 'quality_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>quality_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A radiant module (1) comprising: a first hollow body (3) defined by a front wall (301), a rear wall (302), two sides (303), a first top end (304) having a pair of holes (308) made on the corresponding sides (303) and a bottom end (305) having a pair of holes (308) made on the corresponding sides (303); the first hollow body (3) defines a component, in use, for the passage of a heat carrier fluid, a second hollow body (2) or cover casing inside of which is entirely contained the first hollow body (3); the second hollow body (2) has a front wall (201), a rear wall (202), two sides (203), a top (204), having an air discharge section (B) and a pair of holes (4) made on the corresponding sides (203) and a bottom (210) having an air intake section (A) and a pair of holes (4) made on the corresponding sides (203), the second hollow body (2) is configured for generating an inner zone which is able to define a channel (C2) for controlled flow of the air from the intake section (A) of the bo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An array antenna device of this disclosure includes a substrate, a strip conductor with a linear-shape, which is provided on the substrate, and a power feeder that feeds power to the strip conductor, and a plurality of loop elements, a conductor plate, and a plurality of feeding elements. The plurality of loop elements are provided on a first surface of the substrate, and are located along the strip conductor with a specified spacing from each other. Each of the plurality of loop elements has a loop-shape with a notch. The plurality of feeding elements are connected to the strip conductor, and each has a shape extending along a portion of an outer edge of corresponding one of the plurality of loop elements. The conductor plate is provided on a second surface of the substrate.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A vehicle hood structure capable of improving impact absorption ability prior to a secondary impact when an impacting body impacts the hood. Front end portions 22B of beads 22 of a wave shaped section 20 are arranged in a line in hood plan view. A front wall section 28 is provided further to the hood front side than the wave shaped section 20 and is formed inclined towards the hood bottom side on progression towards the hood front and running substantially along the vehicle width direction. The wave shaped section 20 and the front wall section 28 are connected together by a ledge section 26 formed running substantially along the hood width direction.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This invention relates generally to the generation of antibodies,  e.g. , monoclonal antibodies including fully human monoclonal antibodies, that recognize Jagged 1 and/or Jagged 2, to antibodies,  e.g ., monoclonal antibodies including fully human antibodies that recognize Jagged 1 and/or Jagged 2, and nucleic acid molecules that encode antibodies,  e.g. , nucleic acid molecules that encode monoclonal antibodies including fully human cross-reactive antibodies that recognize both Jagged 1 and Jagged 2, and to methods of making the anti- Jagged antibodies and methods of using the anti- Jagged antibodies as therapeutics, prophylactics, and diagnostics. The invention also relates generally to activatable antibodies that include a masking moiety (MM), a cleavable moiety (CM), and an antibody (AB) that specifically bind to Jagged 1 and Jagged 2, and to methods of making and using these activatable anti- Jagged antibodies in a variety of therapeutic, diagnostic and prophylactic indications.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Methods and implants to treat anterior cruciate ligament (ACL) injuries are disclosed. The methods involve advancing the insertion of the patellar tendon to the proximal tibia by means of a partial osteotomy and a wedge-shaped cage (30). The wedge-shaped cage is specifically designed to facilitate transfer of not only compressive loads, but also of shear loads due to pull by the patellar tendon at its insertion to the tibial tuberosity. The cage decreases the angle between the patellar tendon and the common tangent plane formed by the condyles of the femur and the condyles of the tibia (sometimes called tibial plateau) and consequently modifies the internal joint force, restoring stability to the joint even if the ACL is ruptured. The methods and implants are applicable to both human and canine patients.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0  A radiant module (1) comprising: a first hollow body (3) defined by a front wall (301), a rear wall (302), two sides (303), a first top end (304) having a pair of holes (308) made on the corresponding sides (303) and a bottom end (305) having a pair of holes (308) made on the corresponding sides (303); the first hollow body (3) defines a component, in use, for the passage of a heat carrier fluid, a second hollow body (2) or cover casing inside of which is entirely contained the first hollow body (3); the second hollow body (2) has a front wall (201), a rear wall (202), two sides (203), a top (204), having an air discharge section (B) and a pair of holes (4) made on the corresponding sides (203) and a bottom (210) having an air intake section (A) and a pair of holes (4) made on the corresponding sides (203), the second hollow body (2) is configured for generating an inner zone which is able to define a channel (C2) for controlled flow of the air from the intake section (A) of the bo...   \n",
       "1                                                                                                                                                                                                                       An array antenna device of this disclosure includes a substrate, a strip conductor with a linear-shape, which is provided on the substrate, and a power feeder that feeds power to the strip conductor, and a plurality of loop elements, a conductor plate, and a plurality of feeding elements. The plurality of loop elements are provided on a first surface of the substrate, and are located along the strip conductor with a specified spacing from each other. Each of the plurality of loop elements has a loop-shape with a notch. The plurality of feeding elements are connected to the strip conductor, and each has a shape extending along a portion of an outer edge of corresponding one of the plurality of loop elements. The conductor plate is provided on a second surface of the substrate.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                       A vehicle hood structure capable of improving impact absorption ability prior to a secondary impact when an impacting body impacts the hood. Front end portions 22B of beads 22 of a wave shaped section 20 are arranged in a line in hood plan view. A front wall section 28 is provided further to the hood front side than the wave shaped section 20 and is formed inclined towards the hood bottom side on progression towards the hood front and running substantially along the vehicle width direction. The wave shaped section 20 and the front wall section 28 are connected together by a ledge section 26 formed running substantially along the hood width direction.   \n",
       "3  This invention relates generally to the generation of antibodies,  e.g. , monoclonal antibodies including fully human monoclonal antibodies, that recognize Jagged 1 and/or Jagged 2, to antibodies,  e.g ., monoclonal antibodies including fully human antibodies that recognize Jagged 1 and/or Jagged 2, and nucleic acid molecules that encode antibodies,  e.g. , nucleic acid molecules that encode monoclonal antibodies including fully human cross-reactive antibodies that recognize both Jagged 1 and Jagged 2, and to methods of making the anti- Jagged antibodies and methods of using the anti- Jagged antibodies as therapeutics, prophylactics, and diagnostics. The invention also relates generally to activatable antibodies that include a masking moiety (MM), a cleavable moiety (CM), and an antibody (AB) that specifically bind to Jagged 1 and Jagged 2, and to methods of making and using these activatable anti- Jagged antibodies in a variety of therapeutic, diagnostic and prophylactic indications.   \n",
       "4                                                                                                                                                                                          Methods and implants to treat anterior cruciate ligament (ACL) injuries are disclosed. The methods involve advancing the insertion of the patellar tendon to the proximal tibia by means of a partial osteotomy and a wedge-shaped cage (30). The wedge-shaped cage is specifically designed to facilitate transfer of not only compressive loads, but also of shear loads due to pull by the patellar tendon at its insertion to the tibial tuberosity. The cage decreases the angle between the patellar tendon and the common tangent plane formed by the condyles of the femur and the condyles of the tibia (sometimes called tibial plateau) and consequently modifies the internal joint force, restoring stability to the joint even if the ACL is ruptured. The methods and implants are applicable to both human and canine patients.   \n",
       "\n",
       "   quality_rank  \n",
       "0             0  \n",
       "1             1  \n",
       "2             0  \n",
       "3             1  \n",
       "4             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text            0\n",
       "quality_rank    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(lambda x: sum(x.isnull()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2937\n",
       "1    2063\n",
       "Name: quality_rank, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['quality_rank'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5874"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2937/5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for constructing BERT Embeddings: input_ids, input_masks, input_segments and Inputs\n",
    "MAX_SEQ_LEN=500 # max sequence length\n",
    "\n",
    "def get_masks(tokens):\n",
    "    \"\"\"Masks: 1 for real tokens and 0 for paddings\"\"\"\n",
    "    return [1]*len(tokens) + [0] * (MAX_SEQ_LEN - len(tokens))\n",
    " \n",
    "def get_segments(tokens):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"  \n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (MAX_SEQ_LEN - len(tokens))\n",
    "\n",
    "def get_ids(tokens, tokenizer):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
    "    input_ids = token_ids + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "def create_single_input(sentence, tokenizer, max_len):\n",
    "    \"\"\"Create an input from a sentence\"\"\"\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = stokens[:max_len]\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    " \n",
    "    ids = get_ids(stokens, tokenizer)\n",
    "    masks = get_masks(stokens)\n",
    "    segments = get_segments(stokens)\n",
    "\n",
    "    return ids, masks, segments\n",
    " \n",
    "def convert_sentences_to_features(sentences, tokenizer):\n",
    "    \"\"\"Convert sentences to features: input_ids, input_masks and input_segments\"\"\"\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    " \n",
    "    for sentence in tqdm(sentences,position=0, leave=True):\n",
    "      ids,masks,segments=create_single_input(sentence,tokenizer,MAX_SEQ_LEN-2)\n",
    "      assert len(ids) == MAX_SEQ_LEN\n",
    "      assert len(masks) == MAX_SEQ_LEN\n",
    "      assert len(segments) == MAX_SEQ_LEN\n",
    "      input_ids.append(ids)\n",
    "      input_masks.append(masks)\n",
    "      input_segments.append(segments)\n",
    "\n",
    "    return [np.asarray(input_ids, dtype=np.int32), \n",
    "          np.asarray(input_masks, dtype=np.int32), \n",
    "          np.asarray(input_segments, dtype=np.int32)]\n",
    "\n",
    "def create_tonkenizer(bert_layer):\n",
    "    \"\"\"Instantiate Tokenizer with vocab\"\"\"\n",
    "    vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    do_lower_case=bert_layer.resolved_object.do_lower_case.numpy() \n",
    "    tokenizer=bert.bert_tokenization.FullTokenizer(vocab_file,do_lower_case)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 768)          590592      keras_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 768)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            1538        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 110,074,371\n",
      "Trainable params: 110,074,370\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def bert_model(callable_object):\n",
    "    # Load the pre-trained BERT base model\n",
    "    bert_layer = hub.KerasLayer(handle=callable_object, trainable=True)  \n",
    "   \n",
    "    # BERT layer three inputs: ids, masks and segments\n",
    "    input_ids = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_ids\")           \n",
    "    input_masks = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_masks\")       \n",
    "    input_segments = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    \n",
    "    inputs = [input_ids, input_masks, input_segments] # BERT inputs\n",
    "    pooled_output, sequence_output = bert_layer(inputs) # BERT outputs\n",
    "    \n",
    "    # Add a hidden layer\n",
    "    x = Dense(units=768, activation='relu')(pooled_output)\n",
    "    x = Dropout(0.1)(x)\n",
    " \n",
    "    # Add output layer\n",
    "    outputs = Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    # Construct a new model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = bert_model(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:08<00:00, 454.81it/s]\n",
      "100%|██████████| 1000/1000 [00:02<00:00, 481.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create examples for training and testing\n",
    "df = df.sample(frac=1) # Shuffle the dataset\n",
    "tokenizer = create_tonkenizer(model.layers[3])\n",
    "X_train = convert_sentences_to_features(df['text'][:4000], tokenizer)\n",
    "X_test = convert_sentences_to_features(df['text'][4000:], tokenizer)\n",
    "\n",
    "one_hot_encoded = to_categorical(df['quality_rank'].values)\n",
    "y_train = one_hot_encoded[:4000]\n",
    "y_test =  one_hot_encoded[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:08<00:00, 476.72it/s]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 501.51it/s]\n"
     ]
    }
   ],
   "source": [
    "#df = df.sample(frac=1) # Shuffle the dataset\n",
    "#tokenizer = create_tonkenizer(model.layers[3])\n",
    "#X_train = convert_sentences_to_features(df['text'][:4000], tokenizer)\n",
    "#X_test = convert_sentences_to_features(df['text'][4000:], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_hot_encoded = to_categorical(df['quality_rank'].values)\n",
    "#y_train = one_hot_encoded[:4000]\n",
    "#y_test =  one_hot_encoded[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "\n",
    "# Use Adam optimizer to minimize the categorical_crossentropy loss\n",
    "opt = Adam(learning_rate=2e-5)\n",
    "model.compile(optimizer=opt, \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['acc'])\n",
    "\n",
    "# Fit the data to the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    verbose = 1)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('bert_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "#model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "#          epochs=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
