{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>publication_number</th>\n",
       "      <th>application_number</th>\n",
       "      <th>text</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>new_appl_nbr</th>\n",
       "      <th>appln_id</th>\n",
       "      <th>app_nbr</th>\n",
       "      <th>filing</th>\n",
       "      <th>tech_field</th>\n",
       "      <th>...</th>\n",
       "      <th>breakthrough</th>\n",
       "      <th>breakthrough_xy</th>\n",
       "      <th>generality</th>\n",
       "      <th>originality</th>\n",
       "      <th>radicalness</th>\n",
       "      <th>renewal</th>\n",
       "      <th>quality_index_4</th>\n",
       "      <th>quality_index_6</th>\n",
       "      <th>quality_rank</th>\n",
       "      <th>tech_field_big_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201997</td>\n",
       "      <td>EP-3085354-A1</td>\n",
       "      <td>EP-16167755-A</td>\n",
       "      <td>Provided herein are methods of using gaseous n...</td>\n",
       "      <td>20111215</td>\n",
       "      <td>EP20160167755</td>\n",
       "      <td>451813367</td>\n",
       "      <td>EP20160167755</td>\n",
       "      <td>2011</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880658</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.177162</td>\n",
       "      <td>0.167842</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27936</td>\n",
       "      <td>EP-2510698-A2</td>\n",
       "      <td>EP-10836203-A</td>\n",
       "      <td>Disclosed is a method and apparatus of encodin...</td>\n",
       "      <td>20101208</td>\n",
       "      <td>EP20100836203</td>\n",
       "      <td>334696626</td>\n",
       "      <td>EP20100836203</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690086</td>\n",
       "      <td>0.156627</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.382812</td>\n",
       "      <td>0.432070</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>718991</td>\n",
       "      <td>EP-2927353-A2</td>\n",
       "      <td>EP-15162415-A</td>\n",
       "      <td>The invention relates to an air-jet spinning m...</td>\n",
       "      <td>20150402</td>\n",
       "      <td>EP20150162415</td>\n",
       "      <td>438662266</td>\n",
       "      <td>EP20150162415</td>\n",
       "      <td>2015</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.304487</td>\n",
       "      <td>0.246568</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322964</td>\n",
       "      <td>EP-2731905-A1</td>\n",
       "      <td>EP-12814169-A</td>\n",
       "      <td>A dispensing closure having a closure body and...</td>\n",
       "      <td>20120716</td>\n",
       "      <td>EP20120814169</td>\n",
       "      <td>380483874</td>\n",
       "      <td>EP20120814169</td>\n",
       "      <td>2012</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.744898</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.127407</td>\n",
       "      <td>0.235815</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>788590</td>\n",
       "      <td>EP-2979831-A1</td>\n",
       "      <td>EP-15178742-A</td>\n",
       "      <td>A slitting apparatus having an anvil cylinder ...</td>\n",
       "      <td>20150728</td>\n",
       "      <td>EP20150178742</td>\n",
       "      <td>443003052</td>\n",
       "      <td>EP20150178742</td>\n",
       "      <td>2015</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.338889</td>\n",
       "      <td>0.459559</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>214061</td>\n",
       "      <td>EP-2539441-A1</td>\n",
       "      <td>EP-11704979-A</td>\n",
       "      <td>The present invention relates to a cell cultur...</td>\n",
       "      <td>20110221</td>\n",
       "      <td>EP20110704979</td>\n",
       "      <td>332527837</td>\n",
       "      <td>EP20110704979</td>\n",
       "      <td>2011</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.765306</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.275205</td>\n",
       "      <td>0.231471</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>20029</td>\n",
       "      <td>EP-2483785-A2</td>\n",
       "      <td>EP-10822339-A</td>\n",
       "      <td>The present disclosure includes methods and de...</td>\n",
       "      <td>20100920</td>\n",
       "      <td>EP20100822339</td>\n",
       "      <td>333404856</td>\n",
       "      <td>EP20100822339</td>\n",
       "      <td>2010</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.338130</td>\n",
       "      <td>0.322977</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>636303</td>\n",
       "      <td>EP-2845982-A2</td>\n",
       "      <td>EP-14184100-A</td>\n",
       "      <td>The present invention relates to a post 1 for ...</td>\n",
       "      <td>20140909</td>\n",
       "      <td>EP20140184100</td>\n",
       "      <td>421699870</td>\n",
       "      <td>EP20140184100</td>\n",
       "      <td>2014</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.639053</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.182051</td>\n",
       "      <td>0.240659</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>85513</td>\n",
       "      <td>EP-2470961-A1</td>\n",
       "      <td>EP-10752443-A</td>\n",
       "      <td>Within an area (A 0 ) where of four heads (60 ...</td>\n",
       "      <td>20100824</td>\n",
       "      <td>EP20100752443</td>\n",
       "      <td>323501678</td>\n",
       "      <td>EP20100752443</td>\n",
       "      <td>2010</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.848837</td>\n",
       "      <td>0.639535</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.381746</td>\n",
       "      <td>0.422516</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>590299</td>\n",
       "      <td>EP-3072083-A1</td>\n",
       "      <td>EP-14864337-A</td>\n",
       "      <td>Devices, systems, and techniques are provided ...</td>\n",
       "      <td>20141124</td>\n",
       "      <td>EP20140864337</td>\n",
       "      <td>440613139</td>\n",
       "      <td>EP20140864337</td>\n",
       "      <td>2014</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.407979</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 publication_number application_number  \\\n",
       "0         201997      EP-3085354-A1      EP-16167755-A   \n",
       "1          27936      EP-2510698-A2      EP-10836203-A   \n",
       "2         718991      EP-2927353-A2      EP-15162415-A   \n",
       "3         322964      EP-2731905-A1      EP-12814169-A   \n",
       "4         788590      EP-2979831-A1      EP-15178742-A   \n",
       "...          ...                ...                ...   \n",
       "4995      214061      EP-2539441-A1      EP-11704979-A   \n",
       "4996       20029      EP-2483785-A2      EP-10822339-A   \n",
       "4997      636303      EP-2845982-A2      EP-14184100-A   \n",
       "4998       85513      EP-2470961-A1      EP-10752443-A   \n",
       "4999      590299      EP-3072083-A1      EP-14864337-A   \n",
       "\n",
       "                                                   text  filing_date  \\\n",
       "0     Provided herein are methods of using gaseous n...     20111215   \n",
       "1     Disclosed is a method and apparatus of encodin...     20101208   \n",
       "2     The invention relates to an air-jet spinning m...     20150402   \n",
       "3     A dispensing closure having a closure body and...     20120716   \n",
       "4     A slitting apparatus having an anvil cylinder ...     20150728   \n",
       "...                                                 ...          ...   \n",
       "4995  The present invention relates to a cell cultur...     20110221   \n",
       "4996  The present disclosure includes methods and de...     20100920   \n",
       "4997  The present invention relates to a post 1 for ...     20140909   \n",
       "4998  Within an area (A 0 ) where of four heads (60 ...     20100824   \n",
       "4999  Devices, systems, and techniques are provided ...     20141124   \n",
       "\n",
       "       new_appl_nbr   appln_id        app_nbr  filing  tech_field  ...  \\\n",
       "0     EP20160167755  451813367  EP20160167755    2011        16.0  ...   \n",
       "1     EP20100836203  334696626  EP20100836203    2010         2.0  ...   \n",
       "2     EP20150162415  438662266  EP20150162415    2015        28.0  ...   \n",
       "3     EP20120814169  380483874  EP20120814169    2012        25.0  ...   \n",
       "4     EP20150178742  443003052  EP20150178742    2015        26.0  ...   \n",
       "...             ...        ...            ...     ...         ...  ...   \n",
       "4995  EP20110704979  332527837  EP20110704979    2011        15.0  ...   \n",
       "4996  EP20100822339  333404856  EP20100822339    2010         6.0  ...   \n",
       "4997  EP20140184100  421699870  EP20140184100    2014        35.0  ...   \n",
       "4998  EP20100752443  323501678  EP20100752443    2010         9.0  ...   \n",
       "4999  EP20140864337  440613139  EP20140864337    2014         6.0  ...   \n",
       "\n",
       "      breakthrough  breakthrough_xy  generality  originality  radicalness  \\\n",
       "0              NaN              NaN    0.000000     0.880658     0.240741   \n",
       "1              NaN              NaN    0.000000     0.690086     0.156627   \n",
       "2              NaN              NaN    0.000000          NaN     0.000000   \n",
       "3              NaN              NaN         NaN     0.744898     0.785714   \n",
       "4              NaN              NaN         NaN     0.857143     0.392857   \n",
       "...            ...              ...         ...          ...          ...   \n",
       "4995           NaN              NaN         NaN     0.765306     0.571429   \n",
       "4996           NaN              NaN    0.408163     0.777778     0.208333   \n",
       "4997           NaN              NaN         NaN     0.639053     0.230769   \n",
       "4998           NaN              NaN         NaN     0.848837     0.639535   \n",
       "4999           NaN              NaN    0.277778     0.892857     0.500000   \n",
       "\n",
       "      renewal  quality_index_4  quality_index_6  quality_rank  \\\n",
       "0         8.0         0.177162         0.167842             0   \n",
       "1         9.0         0.382812         0.432070             1   \n",
       "2         3.0         0.304487         0.246568             0   \n",
       "3         6.0         0.127407         0.235815             0   \n",
       "4         3.0         0.338889         0.459559             1   \n",
       "...       ...              ...              ...           ...   \n",
       "4995      8.0         0.275205         0.231471             0   \n",
       "4996      8.0         0.338130         0.322977             1   \n",
       "4997      4.0         0.182051         0.240659             0   \n",
       "4998      8.0         0.381746         0.422516             1   \n",
       "4999      4.0         0.382300         0.407979             1   \n",
       "\n",
       "      tech_field_big_cat  \n",
       "0                      2  \n",
       "1                      0  \n",
       "2                      3  \n",
       "3                      3  \n",
       "4                      3  \n",
       "...                  ...  \n",
       "4995                   2  \n",
       "4996                   0  \n",
       "4997                   4  \n",
       "4998                   1  \n",
       "4999                   0  \n",
       "\n",
       "[5000 rows x 32 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('patent_abstract_5000_2015.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "\n",
    "    (df['quality_index_4'] < 0.3),\n",
    "    (df['quality_index_4'] >= 0.3)\n",
    "]\n",
    "\n",
    "choices = ['0', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['quality_rank'] = np.select(condlist=conditions, choicelist=choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text','quality_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lin_menghsien/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more CNN library\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500  # [Steven] I added for the CNN, to take only the 500 words.\n",
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000  # [Steven] this should be number of unique word / vocabulary\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", str(string))    \n",
    "    string = re.sub(r\"\\'\", \"\", str(string))    \n",
    "    string = re.sub(r\"\\\"\", \"\", str(string))    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>quality_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>The invention relates to a device 20 for detec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>A telescopic extension device for cleaning sys...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>The invention relates to a low-voltage switchi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3867</th>\n",
       "      <td>The invention relates to a method for manufact...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>Embodiments of the present invention disclose ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3046</th>\n",
       "      <td>The present invention relates to a vehicle ped...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>In a direct drill bit drive for tools for comm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4079</th>\n",
       "      <td>The present invention relates to a computer-im...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>The invention relates to a device (25) for pro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>Apparatus for manipulating a color displayed b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text quality_rank\n",
       "27    The invention relates to a device 20 for detec...            0\n",
       "1482  A telescopic extension device for cleaning sys...            0\n",
       "3021  The invention relates to a low-voltage switchi...            0\n",
       "3867  The invention relates to a method for manufact...            1\n",
       "637   Embodiments of the present invention disclose ...            0\n",
       "...                                                 ...          ...\n",
       "3046  The present invention relates to a vehicle ped...            0\n",
       "1725  In a direct drill bit drive for tools for comm...            1\n",
       "4079  The present invention relates to a computer-im...            0\n",
       "2254  The invention relates to a device (25) for pro...            0\n",
       "2915  Apparatus for manipulating a color displayed b...            0\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = df.sample(frac=1, random_state = 5)[:5000]\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "abstracts = [] # abstracts is list of list of list to hold each sentences of each abstract (the most complete data)\n",
    "labels = [] # label is just a list holding our label which is quality_index\n",
    "texts = []  # texts to hold each complete abstract as list of list (note: abstract not breaking up to sentence level)\n",
    "for idx in range(data_train.text.shape[0]): # for each row\n",
    "    #text = BeautifulSoup(data_train.text[idx])\n",
    "    #print(clean_str(str(data_train.iloc[idx]['text'])))\n",
    "    text = clean_str(str(data_train.iloc[idx]['text'])) # text is each complete abstract\n",
    "    texts.append(text) # texts to hold each complete abstract as list of string (note: abstract not breaking up to sentence level)\n",
    "    sentences = tokenize.sent_tokenize(text) # sentences is list of string holding each complete sentence of one abstract (but it's just an intermediate variable, not used directly in later code)\n",
    "    abstracts.append(sentences) # abstracts is list of list of string to hold each sentences of each abstract (the most complete data) (this is what we use )\n",
    "    labels.append(data_train.iloc[idx]['quality_rank']) # label is just a list holding our label which is quality_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS) # intend to use next line .fit_on_texts to index each word within specific abstract at current iteration/loop, the more frequent word has lower index number, it is a dictionary format, it's like a unique vocabulary index\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "# data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17545 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,   26,   42, ...,    0,    0,    0],\n",
       "       [   2, 6771, 1069, ...,    0,    0,    0],\n",
       "       [   1,   26,   42, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,   54,   26, ...,    0,    0,    0],\n",
       "       [   1,   26,   42, ...,    0,    0,    0],\n",
       "       [  85,    9, 6135, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post') # The default is pre-padding, should I try post-padding ? => padding='post', truncating='post'. I think it has to do with whether the beginning words are more important, or later words more important. You may want to try both approaches and compare result?\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 500)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (5000, 500)\n",
      "Shape of label tensor: (5000, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I've already shuffle above using data_train = df.sample(frac=1, random_state = 5)[:5000]\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "# indices = np.arange(data.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "# data = data[indices]\n",
    "# labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "num_validation_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set\n",
      "[2632. 1368.]\n",
      "[651. 349.]\n"
     ]
    }
   ],
   "source": [
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lin_menghsien/team_repo'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import GloVe word embedding\n",
    "GLOVE_DIR = \"\" #\"home/lin_menghsien/w266/project/data/glove\" # I save the glove file in the current team_repo directory\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17546"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  # EMBEDDING_DIM = 100 since we import 100d GloVe\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, datetime, shutil\n",
    "from numpy.random import seed\n",
    "seed(5)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "125/125 [==============================] - 79s 629ms/step - loss: 0.7134 - accuracy: 0.6212 - val_loss: 0.6522 - val_accuracy: 0.6510\n",
      "Epoch 2/15\n",
      "125/125 [==============================] - 77s 619ms/step - loss: 0.6051 - accuracy: 0.6697 - val_loss: 0.6744 - val_accuracy: 0.5750\n",
      "Epoch 3/15\n",
      "125/125 [==============================] - 78s 628ms/step - loss: 0.5223 - accuracy: 0.7440 - val_loss: 0.7634 - val_accuracy: 0.6510\n",
      "Epoch 4/15\n",
      "125/125 [==============================] - 75s 601ms/step - loss: 0.4189 - accuracy: 0.8125 - val_loss: 0.6980 - val_accuracy: 0.6180\n",
      "Epoch 5/15\n",
      "125/125 [==============================] - 72s 579ms/step - loss: 0.2049 - accuracy: 0.9287 - val_loss: 1.0600 - val_accuracy: 0.6550\n",
      "Epoch 6/15\n",
      "125/125 [==============================] - 72s 573ms/step - loss: 0.1270 - accuracy: 0.9572 - val_loss: 1.0802 - val_accuracy: 0.4520\n",
      "Epoch 7/15\n",
      "125/125 [==============================] - 73s 581ms/step - loss: 0.0779 - accuracy: 0.9750 - val_loss: 1.1016 - val_accuracy: 0.6050\n",
      "Epoch 8/15\n",
      "125/125 [==============================] - 72s 572ms/step - loss: 0.0421 - accuracy: 0.9887 - val_loss: 1.4453 - val_accuracy: 0.6410\n",
      "Epoch 9/15\n",
      "125/125 [==============================] - 70s 562ms/step - loss: 0.0531 - accuracy: 0.9847 - val_loss: 1.2754 - val_accuracy: 0.6330\n",
      "Epoch 10/15\n",
      "125/125 [==============================] - 71s 565ms/step - loss: 0.0520 - accuracy: 0.9822 - val_loss: 1.1711 - val_accuracy: 0.5490\n",
      "Epoch 11/15\n",
      "125/125 [==============================] - 70s 560ms/step - loss: 0.0799 - accuracy: 0.9700 - val_loss: 1.3107 - val_accuracy: 0.6050\n",
      "Epoch 12/15\n",
      "125/125 [==============================] - 70s 562ms/step - loss: 0.0546 - accuracy: 0.9812 - val_loss: 1.5770 - val_accuracy: 0.6250\n",
      "Epoch 13/15\n",
      "125/125 [==============================] - 71s 567ms/step - loss: 0.0785 - accuracy: 0.9718 - val_loss: 1.4213 - val_accuracy: 0.6340\n",
      "Epoch 14/15\n",
      "125/125 [==============================] - 71s 566ms/step - loss: 0.0905 - accuracy: 0.9670 - val_loss: 1.5295 - val_accuracy: 0.6310\n",
      "Epoch 15/15\n",
      "125/125 [==============================] - 71s 564ms/step - loss: 0.0507 - accuracy: 0.9847 - val_loss: 1.2645 - val_accuracy: 0.5870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0c9fc732d0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameter setting\n",
    "epochs = 15\n",
    "embed_dim = EMBEDDING_DIM  # already specify with EMBEDDING_DIM variable above\n",
    "dense_layer_dims = [100]\n",
    "dropout_rate = 0.2\n",
    "num_filters = [25, 25, 50, 50, 50]\n",
    "kernel_sizes = [3, 4, 15, 50, 150]\n",
    "num_classes = labels.shape[1] # \n",
    "\n",
    "# Model building\n",
    "start_time = time.time()\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "h = embedding_layer(sequence_input) # remember embedding_layer assign with GloVe's weight\n",
    "\n",
    "conv_layers_for_all_kernel_sizes = []\n",
    "counter = 0\n",
    "for kernel_size, filters in zip(kernel_sizes, num_filters): \n",
    "    conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(h)\n",
    "    conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "    counter += 1\n",
    "    #print(conv_layers_for_all_kernel_sizes)\n",
    "if counter == 1: # need this since layers.concatenate throw error when there is only 1 item in conv_layers_for_all_kernel_sizes\n",
    "    h = conv_layer\n",
    "else:\n",
    "    h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1) # Concat the feature maps from each different size.\n",
    "\n",
    "    h = keras.layers.Dropout(rate=dropout_rate)(h) # Dropout can help with overfitting (improve generalization) by randomly 0-ing different subsets of values in the vector\n",
    "\n",
    "for dim in dense_layer_dims:\n",
    "    h = keras.layers.Dense(dim, activation = 'relu')(h) # [Steven] I believe this add additional fully-connected hidden layer with the hope to increase model accuracy\n",
    "\n",
    "prediction = keras.layers.Dense(num_classes, activation='softmax')(h)\n",
    "\n",
    "model = keras.Model(inputs=sequence_input, outputs=prediction)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',  # From information theory notebooks.\n",
    "              metrics=['accuracy'])        # What metric to output as we train.\n",
    "\n",
    "# train\n",
    "model.reset_states()\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          epochs=epochs, \n",
    "          validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "125/125 [==============================] - 84s 673ms/step - loss: 0.8087 - accuracy: 0.6087 - val_loss: 0.6514 - val_accuracy: 0.6510\n",
      "Epoch 2/15\n",
      "125/125 [==============================] - 83s 662ms/step - loss: 0.5996 - accuracy: 0.6712 - val_loss: 0.6621 - val_accuracy: 0.6200\n",
      "Epoch 3/15\n",
      "125/125 [==============================] - 82s 657ms/step - loss: 0.4876 - accuracy: 0.7740 - val_loss: 0.8527 - val_accuracy: 0.6510\n",
      "Epoch 4/15\n",
      "125/125 [==============================] - 81s 652ms/step - loss: 0.2907 - accuracy: 0.8913 - val_loss: 0.7298 - val_accuracy: 0.6170\n",
      "Epoch 5/15\n",
      "125/125 [==============================] - 80s 641ms/step - loss: 0.1328 - accuracy: 0.9582 - val_loss: 0.9314 - val_accuracy: 0.5190\n",
      "Epoch 6/15\n",
      "125/125 [==============================] - 79s 633ms/step - loss: 0.1328 - accuracy: 0.9485 - val_loss: 0.9492 - val_accuracy: 0.6040\n",
      "Epoch 7/15\n",
      "125/125 [==============================] - 78s 626ms/step - loss: 0.0575 - accuracy: 0.9830 - val_loss: 1.0666 - val_accuracy: 0.5460\n",
      "Epoch 8/15\n",
      "125/125 [==============================] - 76s 608ms/step - loss: 0.0427 - accuracy: 0.9860 - val_loss: 1.5496 - val_accuracy: 0.6380\n",
      "Epoch 9/15\n",
      "125/125 [==============================] - 75s 596ms/step - loss: 0.0376 - accuracy: 0.9877 - val_loss: 1.5105 - val_accuracy: 0.6240\n",
      "Epoch 10/15\n",
      "125/125 [==============================] - 75s 601ms/step - loss: 0.0433 - accuracy: 0.9850 - val_loss: 1.9261 - val_accuracy: 0.6420\n",
      "Epoch 11/15\n",
      "125/125 [==============================] - 75s 599ms/step - loss: 0.0366 - accuracy: 0.9865 - val_loss: 1.7216 - val_accuracy: 0.6250\n",
      "Epoch 12/15\n",
      "125/125 [==============================] - 75s 603ms/step - loss: 0.0171 - accuracy: 0.9958 - val_loss: 1.2987 - val_accuracy: 0.5820\n",
      "Epoch 13/15\n",
      "125/125 [==============================] - 75s 598ms/step - loss: 0.0645 - accuracy: 0.9755 - val_loss: 1.3591 - val_accuracy: 0.5820\n",
      "Epoch 14/15\n",
      "125/125 [==============================] - 75s 596ms/step - loss: 0.0710 - accuracy: 0.9732 - val_loss: 1.3332 - val_accuracy: 0.6050\n",
      "Epoch 15/15\n",
      "125/125 [==============================] - 74s 593ms/step - loss: 0.0856 - accuracy: 0.9710 - val_loss: 1.4514 - val_accuracy: 0.6110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0c780b1490>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameter setting\n",
    "epochs = 15\n",
    "embed_dim = EMBEDDING_DIM  # already specify with EMBEDDING_DIM variable above\n",
    "dense_layer_dims = [100]\n",
    "dropout_rate = 0.2\n",
    "num_filters = [100, 100, 100]\n",
    "kernel_sizes = [48, 50, 52]\n",
    "num_classes = labels.shape[1] # \n",
    "\n",
    "# Model building\n",
    "start_time = time.time()\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "h = embedding_layer(sequence_input) # remember embedding_layer assign with GloVe's weight\n",
    "\n",
    "conv_layers_for_all_kernel_sizes = []\n",
    "counter = 0\n",
    "for kernel_size, filters in zip(kernel_sizes, num_filters): \n",
    "    conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(h)\n",
    "    conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "    counter += 1\n",
    "    #print(conv_layers_for_all_kernel_sizes)\n",
    "if counter == 1: # need this since layers.concatenate throw error when there is only 1 item in conv_layers_for_all_kernel_sizes\n",
    "    h = conv_layer\n",
    "else:\n",
    "    h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1) # Concat the feature maps from each different size.\n",
    "\n",
    "    h = keras.layers.Dropout(rate=dropout_rate)(h) # Dropout can help with overfitting (improve generalization) by randomly 0-ing different subsets of values in the vector\n",
    "\n",
    "for dim in dense_layer_dims:\n",
    "    h = keras.layers.Dense(dim, activation = 'relu')(h) # [Steven] I believe this add additional fully-connected hidden layer with the hope to increase model accuracy\n",
    "\n",
    "prediction = keras.layers.Dense(num_classes, activation='softmax')(h)\n",
    "\n",
    "model = keras.Model(inputs=sequence_input, outputs=prediction)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',  # From information theory notebooks.\n",
    "              metrics=['accuracy'])        # What metric to output as we train.\n",
    "\n",
    "# train\n",
    "model.reset_states()\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          epochs=epochs, \n",
    "          validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "125/125 [==============================] - 69s 551ms/step - loss: 0.7397 - accuracy: 0.6012 - val_loss: 0.6475 - val_accuracy: 0.6490\n",
      "Epoch 2/15\n",
      "125/125 [==============================] - 69s 550ms/step - loss: 0.6156 - accuracy: 0.6653 - val_loss: 0.6459 - val_accuracy: 0.6400\n",
      "Epoch 3/15\n",
      "125/125 [==============================] - 68s 541ms/step - loss: 0.5307 - accuracy: 0.7380 - val_loss: 0.6547 - val_accuracy: 0.6380\n",
      "Epoch 4/15\n",
      "125/125 [==============================] - 68s 545ms/step - loss: 0.4135 - accuracy: 0.8115 - val_loss: 0.7043 - val_accuracy: 0.5770\n",
      "Epoch 5/15\n",
      "125/125 [==============================] - 68s 544ms/step - loss: 0.1943 - accuracy: 0.9367 - val_loss: 0.8352 - val_accuracy: 0.5330\n",
      "Epoch 6/15\n",
      "125/125 [==============================] - 69s 551ms/step - loss: 0.1677 - accuracy: 0.9370 - val_loss: 1.8176 - val_accuracy: 0.3550\n",
      "Epoch 7/15\n",
      "125/125 [==============================] - 69s 548ms/step - loss: 0.1400 - accuracy: 0.9465 - val_loss: 0.9766 - val_accuracy: 0.5180\n",
      "Epoch 8/15\n",
      "125/125 [==============================] - 69s 549ms/step - loss: 0.0500 - accuracy: 0.9858 - val_loss: 1.9314 - val_accuracy: 0.6400\n",
      "Epoch 9/15\n",
      "125/125 [==============================] - 69s 550ms/step - loss: 0.0689 - accuracy: 0.9755 - val_loss: 1.2874 - val_accuracy: 0.6160\n",
      "Epoch 10/15\n",
      "125/125 [==============================] - 70s 562ms/step - loss: 0.0398 - accuracy: 0.9890 - val_loss: 1.5655 - val_accuracy: 0.6370\n",
      "Epoch 11/15\n",
      "125/125 [==============================] - 69s 552ms/step - loss: 0.0496 - accuracy: 0.9803 - val_loss: 1.9750 - val_accuracy: 0.6360\n",
      "Epoch 12/15\n",
      "125/125 [==============================] - 69s 554ms/step - loss: 0.0446 - accuracy: 0.9855 - val_loss: 1.4569 - val_accuracy: 0.5140\n",
      "Epoch 13/15\n",
      "125/125 [==============================] - 70s 556ms/step - loss: 0.0375 - accuracy: 0.9870 - val_loss: 1.3410 - val_accuracy: 0.5810\n",
      "Epoch 14/15\n",
      "125/125 [==============================] - 69s 555ms/step - loss: 0.0203 - accuracy: 0.9935 - val_loss: 1.3339 - val_accuracy: 0.5800\n",
      "Epoch 15/15\n",
      "125/125 [==============================] - 70s 560ms/step - loss: 0.0779 - accuracy: 0.9707 - val_loss: 1.3481 - val_accuracy: 0.5880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0c64e7e290>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameter setting\n",
    "epochs = 15\n",
    "embed_dim = EMBEDDING_DIM  # already specify with EMBEDDING_DIM variable above\n",
    "dense_layer_dims = [100]\n",
    "dropout_rate = 0.2\n",
    "num_filters = [50, 50, 50, 50]\n",
    "kernel_sizes = [5, 20, 55, 125]\n",
    "num_classes = labels.shape[1] # \n",
    "\n",
    "# Model building\n",
    "start_time = time.time()\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "h = embedding_layer(sequence_input) # remember embedding_layer assign with GloVe's weight\n",
    "\n",
    "conv_layers_for_all_kernel_sizes = []\n",
    "counter = 0\n",
    "for kernel_size, filters in zip(kernel_sizes, num_filters): \n",
    "    conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(h)\n",
    "    conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "    counter += 1\n",
    "    #print(conv_layers_for_all_kernel_sizes)\n",
    "if counter == 1: # need this since layers.concatenate throw error when there is only 1 item in conv_layers_for_all_kernel_sizes\n",
    "    h = conv_layer\n",
    "else:\n",
    "    h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1) # Concat the feature maps from each different size.\n",
    "\n",
    "    h = keras.layers.Dropout(rate=dropout_rate)(h) # Dropout can help with overfitting (improve generalization) by randomly 0-ing different subsets of values in the vector\n",
    "\n",
    "for dim in dense_layer_dims:\n",
    "    h = keras.layers.Dense(dim, activation = 'relu')(h) # [Steven] I believe this add additional fully-connected hidden layer with the hope to increase model accuracy\n",
    "\n",
    "prediction = keras.layers.Dense(num_classes, activation='softmax')(h)\n",
    "\n",
    "model = keras.Model(inputs=sequence_input, outputs=prediction)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',  # From information theory notebooks.\n",
    "              metrics=['accuracy'])        # What metric to output as we train.\n",
    "\n",
    "# train\n",
    "model.reset_states()\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          epochs=epochs, \n",
    "          validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "125/125 [==============================] - 92s 737ms/step - loss: 0.7335 - accuracy: 0.6152 - val_loss: 0.6584 - val_accuracy: 0.6510\n",
      "Epoch 2/15\n",
      "125/125 [==============================] - 93s 740ms/step - loss: 0.6127 - accuracy: 0.6668 - val_loss: 0.6508 - val_accuracy: 0.6380\n",
      "Epoch 3/15\n",
      "125/125 [==============================] - 93s 742ms/step - loss: 0.5258 - accuracy: 0.7393 - val_loss: 0.9243 - val_accuracy: 0.6520\n",
      "Epoch 4/15\n",
      "125/125 [==============================] - 91s 728ms/step - loss: 0.3800 - accuracy: 0.8380 - val_loss: 0.7439 - val_accuracy: 0.5790\n",
      "Epoch 5/15\n",
      "125/125 [==============================] - 92s 737ms/step - loss: 0.1510 - accuracy: 0.9530 - val_loss: 0.8543 - val_accuracy: 0.5800\n",
      "Epoch 6/15\n",
      "125/125 [==============================] - 92s 736ms/step - loss: 0.1092 - accuracy: 0.9638 - val_loss: 1.0668 - val_accuracy: 0.6320\n",
      "Epoch 7/15\n",
      "125/125 [==============================] - 89s 714ms/step - loss: 0.0800 - accuracy: 0.9728 - val_loss: 1.1525 - val_accuracy: 0.5140\n",
      "Epoch 8/15\n",
      "125/125 [==============================] - 89s 709ms/step - loss: 0.0753 - accuracy: 0.9753 - val_loss: 1.5373 - val_accuracy: 0.6420\n",
      "Epoch 9/15\n",
      "125/125 [==============================] - 88s 707ms/step - loss: 0.0463 - accuracy: 0.9872 - val_loss: 1.1953 - val_accuracy: 0.6090\n",
      "Epoch 10/15\n",
      "125/125 [==============================] - 88s 704ms/step - loss: 0.0370 - accuracy: 0.9895 - val_loss: 1.2735 - val_accuracy: 0.5950\n",
      "Epoch 11/15\n",
      "125/125 [==============================] - 88s 703ms/step - loss: 0.0452 - accuracy: 0.9845 - val_loss: 1.3869 - val_accuracy: 0.6230\n",
      "Epoch 12/15\n",
      "125/125 [==============================] - 87s 695ms/step - loss: 0.0501 - accuracy: 0.9847 - val_loss: 2.1797 - val_accuracy: 0.6440\n",
      "Epoch 13/15\n",
      "125/125 [==============================] - 88s 702ms/step - loss: 0.0375 - accuracy: 0.9872 - val_loss: 1.4023 - val_accuracy: 0.5430\n",
      "Epoch 14/15\n",
      "125/125 [==============================] - 85s 681ms/step - loss: 0.0921 - accuracy: 0.9622 - val_loss: 1.4826 - val_accuracy: 0.5140\n",
      "Epoch 15/15\n",
      "125/125 [==============================] - 84s 673ms/step - loss: 0.0480 - accuracy: 0.9835 - val_loss: 1.3934 - val_accuracy: 0.5070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0c642cc450>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameter setting\n",
    "epochs = 15\n",
    "embed_dim = EMBEDDING_DIM  # already specify with EMBEDDING_DIM variable above\n",
    "dense_layer_dims = [100]\n",
    "dropout_rate = 0.2\n",
    "num_filters = [100, 100, 100]\n",
    "kernel_sizes = [5, 50, 150]\n",
    "num_classes = labels.shape[1] # \n",
    "\n",
    "# Model building\n",
    "start_time = time.time()\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "h = embedding_layer(sequence_input) # remember embedding_layer assign with GloVe's weight\n",
    "\n",
    "conv_layers_for_all_kernel_sizes = []\n",
    "counter = 0\n",
    "for kernel_size, filters in zip(kernel_sizes, num_filters): \n",
    "    conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(h)\n",
    "    conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "    counter += 1\n",
    "    #print(conv_layers_for_all_kernel_sizes)\n",
    "if counter == 1: # need this since layers.concatenate throw error when there is only 1 item in conv_layers_for_all_kernel_sizes\n",
    "    h = conv_layer\n",
    "else:\n",
    "    h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1) # Concat the feature maps from each different size.\n",
    "\n",
    "    h = keras.layers.Dropout(rate=dropout_rate)(h) # Dropout can help with overfitting (improve generalization) by randomly 0-ing different subsets of values in the vector\n",
    "\n",
    "for dim in dense_layer_dims:\n",
    "    h = keras.layers.Dense(dim, activation = 'relu')(h) # [Steven] I believe this add additional fully-connected hidden layer with the hope to increase model accuracy\n",
    "\n",
    "prediction = keras.layers.Dense(num_classes, activation='softmax')(h)\n",
    "\n",
    "model = keras.Model(inputs=sequence_input, outputs=prediction)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',  # From information theory notebooks.\n",
    "              metrics=['accuracy'])        # What metric to output as we train.\n",
    "\n",
    "# train\n",
    "model.reset_states()\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          epochs=epochs, \n",
    "          validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
