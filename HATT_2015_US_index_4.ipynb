{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-storage\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep only quality_index_6 >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge_quality = pd.read_csv('US_patent_abstract_5000_2015_with_title_1.csv')\n",
    "df_merge_quality.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9117351596366192"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df_merge_quality['quality_index_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0212197159565581"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df_merge_quality['quality_index_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_merge_quality['quality_index_4'], density=False)  # `density=False` would make counts\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Quality_Index_4');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "\n",
    "    (df_merge_quality['quality_index_4'] < 0.3),\n",
    "    (df_merge_quality['quality_index_4'] >= 0.3)\n",
    "]\n",
    "\n",
    "choices = ['0', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_merge_quality['quality_rank'] = np.select(condlist=conditions, choicelist=choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>publication_number</th>\n",
       "      <th>application_number</th>\n",
       "      <th>text</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>new_appl_nbr</th>\n",
       "      <th>appln_id</th>\n",
       "      <th>app_nbr</th>\n",
       "      <th>filing</th>\n",
       "      <th>tech_field</th>\n",
       "      <th>...</th>\n",
       "      <th>breakthrough</th>\n",
       "      <th>breakthrough_xy</th>\n",
       "      <th>generality</th>\n",
       "      <th>originality</th>\n",
       "      <th>radicalness</th>\n",
       "      <th>renewal</th>\n",
       "      <th>quality_index_4</th>\n",
       "      <th>quality_index_6</th>\n",
       "      <th>quality_rank</th>\n",
       "      <th>tech_field_big_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201997</td>\n",
       "      <td>EP-3085354-A1</td>\n",
       "      <td>EP-16167755-A</td>\n",
       "      <td>Provided herein are methods of using gaseous n...</td>\n",
       "      <td>20111215</td>\n",
       "      <td>EP20160167755</td>\n",
       "      <td>451813367</td>\n",
       "      <td>EP20160167755</td>\n",
       "      <td>2011</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880658</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.177162</td>\n",
       "      <td>0.167842</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27936</td>\n",
       "      <td>EP-2510698-A2</td>\n",
       "      <td>EP-10836203-A</td>\n",
       "      <td>Disclosed is a method and apparatus of encodin...</td>\n",
       "      <td>20101208</td>\n",
       "      <td>EP20100836203</td>\n",
       "      <td>334696626</td>\n",
       "      <td>EP20100836203</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690086</td>\n",
       "      <td>0.156627</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.382812</td>\n",
       "      <td>0.432070</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>718991</td>\n",
       "      <td>EP-2927353-A2</td>\n",
       "      <td>EP-15162415-A</td>\n",
       "      <td>The invention relates to an air-jet spinning m...</td>\n",
       "      <td>20150402</td>\n",
       "      <td>EP20150162415</td>\n",
       "      <td>438662266</td>\n",
       "      <td>EP20150162415</td>\n",
       "      <td>2015</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.304487</td>\n",
       "      <td>0.246568</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322964</td>\n",
       "      <td>EP-2731905-A1</td>\n",
       "      <td>EP-12814169-A</td>\n",
       "      <td>A dispensing closure having a closure body and...</td>\n",
       "      <td>20120716</td>\n",
       "      <td>EP20120814169</td>\n",
       "      <td>380483874</td>\n",
       "      <td>EP20120814169</td>\n",
       "      <td>2012</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.744898</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.127407</td>\n",
       "      <td>0.235815</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>788590</td>\n",
       "      <td>EP-2979831-A1</td>\n",
       "      <td>EP-15178742-A</td>\n",
       "      <td>A slitting apparatus having an anvil cylinder ...</td>\n",
       "      <td>20150728</td>\n",
       "      <td>EP20150178742</td>\n",
       "      <td>443003052</td>\n",
       "      <td>EP20150178742</td>\n",
       "      <td>2015</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.338889</td>\n",
       "      <td>0.459559</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>214061</td>\n",
       "      <td>EP-2539441-A1</td>\n",
       "      <td>EP-11704979-A</td>\n",
       "      <td>The present invention relates to a cell cultur...</td>\n",
       "      <td>20110221</td>\n",
       "      <td>EP20110704979</td>\n",
       "      <td>332527837</td>\n",
       "      <td>EP20110704979</td>\n",
       "      <td>2011</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.765306</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.275205</td>\n",
       "      <td>0.231471</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>20029</td>\n",
       "      <td>EP-2483785-A2</td>\n",
       "      <td>EP-10822339-A</td>\n",
       "      <td>The present disclosure includes methods and de...</td>\n",
       "      <td>20100920</td>\n",
       "      <td>EP20100822339</td>\n",
       "      <td>333404856</td>\n",
       "      <td>EP20100822339</td>\n",
       "      <td>2010</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.338130</td>\n",
       "      <td>0.322977</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>636303</td>\n",
       "      <td>EP-2845982-A2</td>\n",
       "      <td>EP-14184100-A</td>\n",
       "      <td>The present invention relates to a post 1 for ...</td>\n",
       "      <td>20140909</td>\n",
       "      <td>EP20140184100</td>\n",
       "      <td>421699870</td>\n",
       "      <td>EP20140184100</td>\n",
       "      <td>2014</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.639053</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.182051</td>\n",
       "      <td>0.240659</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>85513</td>\n",
       "      <td>EP-2470961-A1</td>\n",
       "      <td>EP-10752443-A</td>\n",
       "      <td>Within an area (A 0 ) where of four heads (60 ...</td>\n",
       "      <td>20100824</td>\n",
       "      <td>EP20100752443</td>\n",
       "      <td>323501678</td>\n",
       "      <td>EP20100752443</td>\n",
       "      <td>2010</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.848837</td>\n",
       "      <td>0.639535</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.381746</td>\n",
       "      <td>0.422516</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>590299</td>\n",
       "      <td>EP-3072083-A1</td>\n",
       "      <td>EP-14864337-A</td>\n",
       "      <td>Devices, systems, and techniques are provided ...</td>\n",
       "      <td>20141124</td>\n",
       "      <td>EP20140864337</td>\n",
       "      <td>440613139</td>\n",
       "      <td>EP20140864337</td>\n",
       "      <td>2014</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.407979</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 publication_number application_number  \\\n",
       "0         201997      EP-3085354-A1      EP-16167755-A   \n",
       "1          27936      EP-2510698-A2      EP-10836203-A   \n",
       "2         718991      EP-2927353-A2      EP-15162415-A   \n",
       "3         322964      EP-2731905-A1      EP-12814169-A   \n",
       "4         788590      EP-2979831-A1      EP-15178742-A   \n",
       "...          ...                ...                ...   \n",
       "4995      214061      EP-2539441-A1      EP-11704979-A   \n",
       "4996       20029      EP-2483785-A2      EP-10822339-A   \n",
       "4997      636303      EP-2845982-A2      EP-14184100-A   \n",
       "4998       85513      EP-2470961-A1      EP-10752443-A   \n",
       "4999      590299      EP-3072083-A1      EP-14864337-A   \n",
       "\n",
       "                                                   text  filing_date  \\\n",
       "0     Provided herein are methods of using gaseous n...     20111215   \n",
       "1     Disclosed is a method and apparatus of encodin...     20101208   \n",
       "2     The invention relates to an air-jet spinning m...     20150402   \n",
       "3     A dispensing closure having a closure body and...     20120716   \n",
       "4     A slitting apparatus having an anvil cylinder ...     20150728   \n",
       "...                                                 ...          ...   \n",
       "4995  The present invention relates to a cell cultur...     20110221   \n",
       "4996  The present disclosure includes methods and de...     20100920   \n",
       "4997  The present invention relates to a post 1 for ...     20140909   \n",
       "4998  Within an area (A 0 ) where of four heads (60 ...     20100824   \n",
       "4999  Devices, systems, and techniques are provided ...     20141124   \n",
       "\n",
       "       new_appl_nbr   appln_id        app_nbr  filing  tech_field  ...  \\\n",
       "0     EP20160167755  451813367  EP20160167755    2011        16.0  ...   \n",
       "1     EP20100836203  334696626  EP20100836203    2010         2.0  ...   \n",
       "2     EP20150162415  438662266  EP20150162415    2015        28.0  ...   \n",
       "3     EP20120814169  380483874  EP20120814169    2012        25.0  ...   \n",
       "4     EP20150178742  443003052  EP20150178742    2015        26.0  ...   \n",
       "...             ...        ...            ...     ...         ...  ...   \n",
       "4995  EP20110704979  332527837  EP20110704979    2011        15.0  ...   \n",
       "4996  EP20100822339  333404856  EP20100822339    2010         6.0  ...   \n",
       "4997  EP20140184100  421699870  EP20140184100    2014        35.0  ...   \n",
       "4998  EP20100752443  323501678  EP20100752443    2010         9.0  ...   \n",
       "4999  EP20140864337  440613139  EP20140864337    2014         6.0  ...   \n",
       "\n",
       "      breakthrough  breakthrough_xy  generality  originality  radicalness  \\\n",
       "0              NaN              NaN    0.000000     0.880658     0.240741   \n",
       "1              NaN              NaN    0.000000     0.690086     0.156627   \n",
       "2              NaN              NaN    0.000000          NaN     0.000000   \n",
       "3              NaN              NaN         NaN     0.744898     0.785714   \n",
       "4              NaN              NaN         NaN     0.857143     0.392857   \n",
       "...            ...              ...         ...          ...          ...   \n",
       "4995           NaN              NaN         NaN     0.765306     0.571429   \n",
       "4996           NaN              NaN    0.408163     0.777778     0.208333   \n",
       "4997           NaN              NaN         NaN     0.639053     0.230769   \n",
       "4998           NaN              NaN         NaN     0.848837     0.639535   \n",
       "4999           NaN              NaN    0.277778     0.892857     0.500000   \n",
       "\n",
       "      renewal  quality_index_4  quality_index_6  quality_rank  \\\n",
       "0         8.0         0.177162         0.167842             0   \n",
       "1         9.0         0.382812         0.432070             1   \n",
       "2         3.0         0.304487         0.246568             1   \n",
       "3         6.0         0.127407         0.235815             0   \n",
       "4         3.0         0.338889         0.459559             1   \n",
       "...       ...              ...              ...           ...   \n",
       "4995      8.0         0.275205         0.231471             0   \n",
       "4996      8.0         0.338130         0.322977             1   \n",
       "4997      4.0         0.182051         0.240659             0   \n",
       "4998      8.0         0.381746         0.422516             1   \n",
       "4999      4.0         0.382300         0.407979             1   \n",
       "\n",
       "      tech_field_big_cat  \n",
       "0                      2  \n",
       "1                      0  \n",
       "2                      3  \n",
       "3                      3  \n",
       "4                      3  \n",
       "...                  ...  \n",
       "4995                   2  \n",
       "4996                   0  \n",
       "4997                   4  \n",
       "4998                   1  \n",
       "4999                   0  \n",
       "\n",
       "[5000 rows x 32 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 30\n",
    "MAX_SENTS = 10\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = df_merge_quality.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2896\n",
       "1    2104\n",
       "Name: quality_rank, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['quality_rank'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5792"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2896/5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/nobu_yamaguchi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "abstracts = []\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for idx in range(data_train.text.shape[0]):\n",
    "\n",
    "    text = clean_str(str(data_train.iloc[idx]['claims_text']))\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    abstracts.append(sentences)\n",
    "    labels.append(data_train.iloc[idx]['quality_rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10, 30)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentences in enumerate(abstracts):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "            #for word in wordTokens:\n",
    "                if word in tokenizer.word_index.keys():\n",
    "                    if (k < MAX_SENT_LENGTH) and (tokenizer.word_index[word] < MAX_NB_WORDS):\n",
    "                        data[i, j, k] = tokenizer.word_index[word]\n",
    "                        k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31722 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (5000, 10, 30)\n",
      "Shape of label tensor: (5000, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set\n",
      "[2323. 1677.]\n",
      "[573. 427.]\n"
     ]
    }
   ],
   "source": [
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard python helper libraries.\n",
    "import os, sys, re, json, time\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display\n",
    "\n",
    "# NumPy and SciPy for matrix ops\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "\n",
    "# Helper libraries\n",
    "#from w266_common import utils, vocabulary, tf_embed_viz\n",
    "#import glove_helper; reload(glove_helper)\n",
    "\n",
    "#hands = glove_helper.Hands(ndim=100)  # 50, 100, 200, 300 dim are available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"data/glove/glove.6B.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"data/glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = \"data/glove\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building Hierachical Attention network\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)), name='W')\n",
    "        self.b = K.variable(self.init((self.attention_dim, )), name='b')\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)), name='u')\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    " \n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b)) #\n",
    "\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "\n",
    "        #if mask is not None:\n",
    "        #    # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "        #    print('mask')\n",
    "        #    ait *= K.cast(mask, K.floatx())\n",
    "\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        ait = K.expand_dims(ait)\n",
    " \n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'att_layer_5/W:0' shape=(200, 100) dtype=float32> W\n",
      "tracking <tf.Variable 'att_layer_5/b:0' shape=(100,) dtype=float32> b\n",
      "tracking <tf.Variable 'att_layer_5/u:0' shape=(100, 1) dtype=float32> u\n"
     ]
    }
   ],
   "source": [
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "l_lstm = Bidirectional(GRU(100, return_sequences=True))(embedded_sequences)\n",
    "l_att = AttLayer(100)(l_lstm)\n",
    "sentEncoder = Model(sentence_input, l_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "abstract_encoder = TimeDistributed(sentEncoder)(abstract_input)\n",
    "l_lstm_sent = Bidirectional(GRU(30, return_sequences=True))(abstract_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 3, 30])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 3, 200])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_encoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 3, 60])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_lstm_sent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'att_layer_6/W:0' shape=(60, 100) dtype=float32> W\n",
      "tracking <tf.Variable 'att_layer_6/b:0' shape=(100,) dtype=float32> b\n",
      "tracking <tf.Variable 'att_layer_6/u:0' shape=(100, 1) dtype=float32> u\n"
     ]
    }
   ],
   "source": [
    "#l_att_sent = AttLayer(100)(l_lstm_sent)\n",
    "l_att_sent = AttLayer(100)(l_lstm_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_att_dense = Dense(100, activation='relu')(l_att_sent)\n",
    "l_att_dense = Dense(100, activation='relu')(l_att_dense)\n",
    "l_att_dense = Dense(100, activation='relu')(l_att_dense)\n",
    "l_att_sent_drop = Dropout(rate=0.3)(l_att_dense)\n",
    "preds = Dense(2, activation='sigmoid')(l_att_sent_drop)\n",
    "model = Model(abstract_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nobu_yamaguchi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 14s 4ms/step - loss: 0.6817 - acc: 0.5739 - val_loss: 0.6860 - val_acc: 0.5640\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.6767 - acc: 0.5831 - val_loss: 0.6846 - val_acc: 0.5640\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.6757 - acc: 0.5825 - val_loss: 0.6838 - val_acc: 0.5650\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.6718 - acc: 0.5804 - val_loss: 0.6839 - val_acc: 0.5675\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.6601 - acc: 0.6060 - val_loss: 0.6884 - val_acc: 0.5385\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.6471 - acc: 0.6164 - val_loss: 0.6821 - val_acc: 0.5445\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.6333 - acc: 0.6395 - val_loss: 0.6862 - val_acc: 0.5665\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.6077 - acc: 0.6645 - val_loss: 0.7015 - val_acc: 0.5770\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.5750 - acc: 0.6970 - val_loss: 0.7307 - val_acc: 0.5595\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 11s 3ms/step - loss: 0.5205 - acc: 0.7451 - val_loss: 0.7485 - val_acc: 0.5790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f775bb8d990>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"model fitting - Hierachical attention network 0.3\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We changed max_sentence to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network - 0.3 Max sentence = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nobu_yamaguchi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "4000/4000 [==============================] - 32s 8ms/step - loss: 0.6586 - acc: 0.5811 - val_loss: 0.6518 - val_acc: 0.5730\n",
      "Epoch 2/10\n",
      "4000/4000 [==============================] - 27s 7ms/step - loss: 0.6520 - acc: 0.5786 - val_loss: 0.6519 - val_acc: 0.5845\n",
      "Epoch 3/10\n",
      "4000/4000 [==============================] - 27s 7ms/step - loss: 0.6512 - acc: 0.5895 - val_loss: 0.6475 - val_acc: 0.6060\n",
      "Epoch 4/10\n",
      "4000/4000 [==============================] - 28s 7ms/step - loss: 0.6464 - acc: 0.6056 - val_loss: 0.6496 - val_acc: 0.5830\n",
      "Epoch 5/10\n",
      "4000/4000 [==============================] - 28s 7ms/step - loss: 0.6381 - acc: 0.6302 - val_loss: 0.6447 - val_acc: 0.6110\n",
      "Epoch 6/10\n",
      "4000/4000 [==============================] - 28s 7ms/step - loss: 0.6363 - acc: 0.6341 - val_loss: 0.6381 - val_acc: 0.6320\n",
      "Epoch 7/10\n",
      "4000/4000 [==============================] - 27s 7ms/step - loss: 0.6231 - acc: 0.6515 - val_loss: 0.6368 - val_acc: 0.6300\n",
      "Epoch 8/10\n",
      "4000/4000 [==============================] - 27s 7ms/step - loss: 0.6104 - acc: 0.6668 - val_loss: 0.6373 - val_acc: 0.6450\n",
      "Epoch 9/10\n",
      "4000/4000 [==============================] - 27s 7ms/step - loss: 0.6034 - acc: 0.6745 - val_loss: 0.6307 - val_acc: 0.6305\n",
      "Epoch 10/10\n",
      "4000/4000 [==============================] - 28s 7ms/step - loss: 0.5729 - acc: 0.7032 - val_loss: 0.6265 - val_acc: 0.6445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f779de7ed50>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"model fitting - Hierachical attention network - 0.3 Max sentence = 10\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=10, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network  .3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nobu_yamaguchi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "4000/4000 [==============================] - 13s 3ms/step - loss: 0.6465 - acc: 0.6551 - val_loss: 0.6522 - val_acc: 0.6390\n",
      "Epoch 2/5\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.6405 - acc: 0.6610 - val_loss: 0.6508 - val_acc: 0.6390\n",
      "Epoch 3/5\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.6377 - acc: 0.6611 - val_loss: 0.6483 - val_acc: 0.6390\n",
      "Epoch 4/5\n",
      "4000/4000 [==============================] - 9s 2ms/step - loss: 0.6330 - acc: 0.6599 - val_loss: 0.6472 - val_acc: 0.6370\n",
      "Epoch 5/5\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.6250 - acc: 0.6629 - val_loss: 0.6455 - val_acc: 0.6395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f87a01a0b10>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"model fitting - Hierachical attention network  .3\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          nb_epoch=5, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
