{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-for-tf2\n",
      "  Downloading bert-for-tf2-0.14.4.tar.gz (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 1.7 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting py-params>=0.9.6\n",
      "  Downloading py-params-0.9.7.tar.gz (6.8 kB)\n",
      "Collecting params-flow>=0.8.0\n",
      "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
      "Requirement already satisfied: numpy in /home/nobu_yamaguchi/anaconda3/lib/python3.7/site-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.1)\n",
      "Requirement already satisfied: tqdm in /home/nobu_yamaguchi/anaconda3/lib/python3.7/site-packages (from params-flow>=0.8.0->bert-for-tf2) (4.42.1)\n",
      "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
      "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.4-py3-none-any.whl size=30114 sha256=8add6b6b4598d4c300f4814767ee1440b896ccacbd5ae05a428af27aa710a1a0\n",
      "  Stored in directory: /home/nobu_yamaguchi/.cache/pip/wheels/6c/c9/9c/363182ea34a736dae336eeaf0dd4a7eec3c6a5afe32373e1fe\n",
      "  Building wheel for py-params (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py-params: filename=py_params-0.9.7-py3-none-any.whl size=7302 sha256=8ac0dcbc968ce8599110cba6d6bb23899f4465fb7eea4584186b35556c449a0a\n",
      "  Stored in directory: /home/nobu_yamaguchi/.cache/pip/wheels/47/3d/2d/bbffcfd6b9f4b8b5cbf07e7520ac2676192fe9431240c13ee8\n",
      "  Building wheel for params-flow (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19473 sha256=db82dd4d94cafc56d4ee6e075c4dc30fc1c29e662d0b70567d39fb5248e18166\n",
      "  Stored in directory: /home/nobu_yamaguchi/.cache/pip/wheels/0e/fc/d2/a44fff33af0f233d7def6e7de413006d57c10e10ad736fe8f5\n",
      "Successfully built bert-for-tf2 py-params params-flow\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
      "Successfully installed bert-for-tf2-0.14.4 params-flow-0.8.2 py-params-0.9.7\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/nobu_yamaguchi/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.91\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/nobu_yamaguchi/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.8.0-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 2.7 MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /home/nobu_yamaguchi/anaconda3/lib/python3.7/site-packages (from tensorflow_hub) (3.12.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/nobu_yamaguchi/anaconda3/lib/python3.7/site-packages (from tensorflow_hub) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/nobu_yamaguchi/anaconda3/lib/python3.7/site-packages (from tensorflow_hub) (1.18.1)\n",
      "Requirement already satisfied: setuptools in /home/nobu_yamaguchi/anaconda3/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow_hub) (45.2.0.post20200210)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.8.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/nobu_yamaguchi/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import numpy as up\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patent_abstract = pd.read_csv('patent_abstract_5000_2015.csv')\n",
    "patent_abstract.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(patent_abstract['fwd_cits5'], density=False)  # `density=False` would make counts\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('fwd_cits5');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "\n",
    "    (patent_abstract['fwd_cits5'] < 1),\n",
    "    (patent_abstract['fwd_cits5'] >= 1)\n",
    "]\n",
    "\n",
    "choices = ['0', '1']\n",
    "\n",
    "patent_abstract['fwd_cits5_rank'] = np.select(condlist=conditions, choicelist=choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = patent_abstract[['text', 'fwd_cits5_rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3459\n",
       "1    1541\n",
       "Name: fwd_cits5_rank, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patent_abstract['fwd_cits5_rank'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = []\n",
    "sentences = list(df['text'])\n",
    "for sen in sentences:\n",
    "    abstracts.append(preprocess_text(str(sen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text' 'fwd_cits5_rank']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '0'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fwd_cits5_rank.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.fwd_cits5_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_abstracts(text_abstracts):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_abstracts = [tokenize_abstracts(abstract) for abstract in abstracts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerparing Data For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_with_len = [[abstract, y[i], len(abstract)]\n",
    "                 for i, abstract in enumerate(tokenized_abstracts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(abstracts_with_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_with_len.sort(key=lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_abstracts_labels = [(abstract_lab[0], abstract_lab[1]) for abstract_lab in abstracts_with_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_abstracts_labels, output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 22), dtype=int32, numpy=\n",
       " array([[16660,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [16660,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 1996, 11028, 14623,  2000,  2019, 14794,  9179, 17564,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 1996,  2556, 12827,  3640,  2019,  7554,  2422, 12495, 13027,\n",
       "          5080,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 1996,  2556, 11028, 14623,  2000,  2132,  2039,  4653,  1999,\n",
       "          5013,  4316,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 2522,  2818, 19738,  2099, 27159,  2132, 11198,  2015,  2007,\n",
       "          5301, 13438, 19120,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 4725,  1998, 14709,  2005, 17731,  8331,  2076,  8153, 28155,\n",
       "         24848,  3370,  7709,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 1996, 11028, 14623,  2000, 16474,  5783,  1998,  2000,  4118,\n",
       "          2005,  1996,  2537, 21739,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 1037,  7328,  1997,  1996,  5675,  1998, 13859,  9265,  2005,\n",
       "          1996,  3949,  1997,  3255,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 1996,  2556, 11028,  3640,  2128,  4270,  7666,  1998,  4725,\n",
       "          2005,  7388,  4456, 10788,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 1996,  2556, 11028,  5936,  1996,  3949,  1997, 23725,  2007,\n",
       "          3653, 29021,  6653,  3384, 16771,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 1996, 11028, 14623,  2000, 25416, 22648,  7062,  2543, 18907,\n",
       "          4254, 11687,  2036,  2170,  4254,  8962,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 1996,  2556, 11028,  2003,  2856,  2000,  2832,  2005,  1996,\n",
       "          7547,  1997,  7328,  1997,  5675, 22038,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 2019, 24177, 18291, 18676,  3459,  3886,  2383,  3872, 12884,\n",
       "          1997, 10768, 18752,  2618,  4403,  1997,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 1996,  2556, 11028, 14623,  2000,  3117, 14017, 16952,  2015,\n",
       "          1997, 18243,  9521, 21663,  1997,  5675,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 1037,  1043,  2135, 13186,  8516,  4651, 11022,  2013,  2822,\n",
       "         10654,  6238,  1998,  3141,  4725,  2024,  2649,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 1996, 11028, 14623,  2000, 25416, 22648,  7062, 14692, 14834,\n",
       "          5318,  1998,  7978, 14692, 25416, 22648,  7062, 14834,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 1037, 17782,  2594,  3430,  2832,  2950,  4852, 12969,  2306,\n",
       "         17782,  2000, 12939,  3563, 12139,  1997, 17782,  8859,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 1996,  3395,  3043, 21362,  2182,  2378, 14623,  2000, 19120,\n",
       "          3001,  1998,  3295,  9128,  2478, 10903, 26035,  2075,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [ 1996, 11028, 14623,  2000,  2152,  3997,  6970, 16643, 20925,\n",
       "          2489,  2659,  4304,  3886,  1998,  4118,  2005,  5155,  2056,\n",
       "          3886,     0,     0,     0],\n",
       "        [ 1996, 11028, 14623,  2000,  7584, 18898,  2489,  6177,  1997,\n",
       "         11641, 15856,  2081,  1997, 10764, 11970,  1998,  1996, 28655,\n",
       "         21739,     0,     0,     0],\n",
       "        [ 1996,  2556, 11028, 14623,  2000,  1996,  2224,  1997, 19857,\n",
       "         28741,  4383, 14175, 18908, 11390,  1997,  5675,  1999, 20739,\n",
       "         22698,     0,     0,     0],\n",
       "        [ 2649,  2003,  2047, 20219,  1997,  8457,  2128,  7559, 28210,\n",
       "          3040, 14479,  8376,  2004,  2092,  2004,  2832,  2005,  2037,\n",
       "          2537,     0,     0,     0],\n",
       "        [ 1996, 11028, 14623,  2000,  3688,  1998,  4725,  2008,  3073,\n",
       "          3445, 16189,  1997,  3056,  2557, 21890, 17830,  3401, 21823,\n",
       "          9289,  2015,     0,     0],\n",
       "        [ 2023, 11028, 14623,  2000,  5512,  2164,  7279, 11031,  8067,\n",
       "          2618,  1052,  3372,  2030, 13859,  5662, 21739,  1998, 13012,\n",
       "         10732,  7629,     0,     0],\n",
       "        [ 1996, 11028,  3640, 12702, 10258, 21272,  2594,  5733,  1998,\n",
       "          4725,  1997,  2478,  2107,  5733,  2005, 22910,  7300,  2107,\n",
       "          2004,  2668,     0,     0],\n",
       "        [ 1037,  5915,  5814,  4118,  2950,  5716,  6994,  2075,  1999,\n",
       "          5915,  5814,  2832,  1996,  6994,  2075,  2003, 15026,  2007,\n",
       "          6204,  3512,  3430,     0],\n",
       "        [ 1996,  2556, 11028, 14623,  2000,  2832,  2005,  1996,  7547,\n",
       "          1997, 17316,  2632,  4801,  7096,  3388, 20409,  4747,  8516,\n",
       "         23060, 14428, 16942,     0],\n",
       "        [ 4874,  1997,  1996,  2556, 11028,  2003,  2019,  5301,  2832,\n",
       "          2005,  1996,  7547,  1997,  3145,  7783,  2015,  2005,  1996,\n",
       "         10752,  1997, 28093,  7076],\n",
       "        [ 1996, 11028, 14623,  2000,  4118,  2005, 12515,  3635,  1043,\n",
       "          1997, 18093,  3344,  2011,  2019,  4910,  2147,  4316,  3081,\n",
       "          2093,  2391,  4957,  4270],\n",
       "        [ 1996, 11028, 14623,  2000,  3117, 18898,  5080,  1996,  4118,\n",
       "          3344,  2041,  2478,  2056,  5080,  3688,  4663,  8558,  1998,\n",
       "          2224,  1997,  2056,  3688],\n",
       "        [ 1996, 11028, 14623,  2000, 24471, 11031,  7231,  4748, 21579,\n",
       "          5512,  2383,  2152, 16913, 11627,  1998,  2003, 10216,  4651,\n",
       "          3085,  2012,  2282,  4860]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0], dtype=int32)>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(batched_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_BATCHES = np.math.ceil(len(sorted_abstracts_labels) / BATCH_SIZE)\n",
    "TEST_BATCHES = TOTAL_BATCHES // 20\n",
    "batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "test_data = batched_dataset.take(TEST_BATCHES)\n",
    "train_data = batched_dataset.skip(TEST_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=2,\n",
    "                 dropout_rate=0.3,\n",
    "                 training=False,\n",
    "                 name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3) \n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                        embedding_dimensions=EMB_DIM,\n",
    "                        cnn_filters=CNN_FILTERS,\n",
    "                        dnn_units=DNN_UNITS,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_CLASSES == 2:\n",
    "    text_model.compile(loss=\"binary_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"acc\"])\n",
    "else:\n",
    "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "150/150 [==============================] - 15s 97ms/step - loss: 0.6223 - acc: 0.6897\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 13s 89ms/step - loss: 0.5780 - acc: 0.6981\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 14s 92ms/step - loss: 0.3091 - acc: 0.8790\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 14s 92ms/step - loss: 0.0452 - acc: 0.9929\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 14s 92ms/step - loss: 0.0130 - acc: 0.9969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff5caf711d0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.fit(train_data, epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7/Unknown - 0s 25ms/step - loss: 1.4123 - acc: 0.5045[1.412281104496547, 0.50446427]\n"
     ]
    }
   ],
   "source": [
    "results = text_model.evaluate(test_data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
