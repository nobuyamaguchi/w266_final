{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "C3iwMmXgIcOk",
    "outputId": "82895b42-eb33-40da-91ca-f03d257f5658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-for-tf2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/df/ab6d927d6162657f30eb0ae3c534c723c28c191a9caf6ee68ec935df3d0b/bert-for-tf2-0.14.5.tar.gz (40kB)\n",
      "\r",
      "\u001b[K     |████████                        | 10kB 24.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 20kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 30kB 4.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 40kB 2.7MB/s \n",
      "\u001b[?25hCollecting py-params>=0.9.6\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
      "Collecting params-flow>=0.8.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
      "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
      "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.5-cp36-none-any.whl size=30315 sha256=64c04f7d1ed53c568bf5f37944f44d4b001cf84f017accf1b68fa9f33b3fdc0e\n",
      "  Stored in directory: /root/.cache/pip/wheels/2e/70/a2/be357037dd2cbdcaeb0add1fdf083be6a600ca65ee1f68751c\n",
      "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=3d32d4759989c5078c3316aa11233081b872a182cf60bb6c7afa4609f4f9cc97\n",
      "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
      "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19473 sha256=71eace5a8a1528fe5d3674e16aa8c54a697a331460f19c0e6da6d54d99a24c4e\n",
      "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
      "Successfully built bert-for-tf2 py-params params-flow\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
      "Successfully installed bert-for-tf2-0.14.5 params-flow-0.8.2 py-params-0.9.7\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 4.7MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.91\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXToONeAHfQ-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as up\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "import random\n",
    "\n",
    "from google.colab import drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gaPuj5Q0HfRC",
    "outputId": "3239d7cd-803a-45a9-9383-ac16579895c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N8PCKEY5HfRG",
    "outputId": "f203e899-dfe5-4f40-85cf-9fda77d0b504",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eQ1eI-yVHfRJ",
    "outputId": "89f55d8a-f975-47ab-b00f-5a56f1bf2be7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "# string processing\n",
    "import re\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing, feature_selection, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "8Za7leNhKTFd",
    "outputId": "d55c9b11-9d0e-406e-c439-42ef5fa8ae0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U3k_fdE9QmZz",
    "outputId": "f26101e1-021a-4091-c253-2e6ace857718"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7hUzAAluKuWX"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_merge_quality = pd.read_csv(\n",
    "    \"/content/drive/My Drive/NLP_Udemy/data/US_patent_abstract_50000_2015_with_title_1_5y.csv\",\n",
    " #   header=None,\n",
    " #   names=cols,\n",
    "    engine=\"python\",\n",
    "    encoding=\"latin1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "OtB-WCOFHfRO",
    "outputId": "fb7ee849-2fd4-4bfe-80b1-f429392c2cc3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claims_text</th>\n",
       "      <th>quality_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is claimed is: \\n     \\n       1. A user ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I claim: \\n     \\n       1. A mechanism for as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We claim: \\n     \\n       1. An aircraft landi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1. A method for culturing primate pluripotent ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is claimed is: \\n     \\n       1. A metho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>What is claimed is: \\n     \\n       1. A sched...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>What is claimed is: \\n     \\n       1. An appa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>What is claimed is: \\n     \\n       1. An imag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I claim: \\n     \\n       1. A method comprisin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>What is claimed is: \\n     \\n       1. An inst...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             claims_text  quality_rank\n",
       "0      What is claimed is: \\n     \\n       1. A user ...             1\n",
       "1      I claim: \\n     \\n       1. A mechanism for as...             0\n",
       "2      We claim: \\n     \\n       1. An aircraft landi...             1\n",
       "3      1. A method for culturing primate pluripotent ...             1\n",
       "4      What is claimed is: \\n     \\n       1. A metho...             0\n",
       "...                                                  ...           ...\n",
       "49995  What is claimed is: \\n     \\n       1. A sched...             0\n",
       "49996  What is claimed is: \\n     \\n       1. An appa...             0\n",
       "49997  What is claimed is: \\n     \\n       1. An imag...             1\n",
       "49998  I claim: \\n     \\n       1. A method comprisin...             0\n",
       "49999  What is claimed is: \\n     \\n       1. An inst...             0\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_merge_quality[['claims_text', 'quality_rank']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nERKyWlRHfRQ"
   },
   "source": [
    "### Prep for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uWkOPJMtHfRR"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wt8Jv5_qHfRT"
   },
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uvUASSgRHfRW"
   },
   "outputs": [],
   "source": [
    "claims = []\n",
    "sentences = list(df['claims_text'])\n",
    "for sen in sentences:\n",
    "    claims.append(preprocess_text(str(sen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "QlyPe9_XHfRY",
    "outputId": "21ae5750-3589-4331-85e0-e602472585af",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'What is claimed is user equipment UE device the UE device comprising an antenna for performing wireless communications with base station memory which stores plurality of sets of communication scenario information for each of plurality of UE communication scenarios wherein each of the stored plurality of sets of communication scenario information is based on two or more of receiver type multiple input multiple output MIMO scheme and amount of Doppler shift wherein each set of communication scenario information is useable in generating channel quality indicator for respective communication scenario processor configured to determine current communication scenario of the UE device during operation of the UE wherein determining the current communication scenario is based on the two or more of receiver type MIMO scheme and amount of Doppler shift select first set of communication scenario information from the plurality of sets of communication scenario information based on the determined current communication scenario of the UE device determine current channel estimation for the UE device and generate at least one channel quality indicator based on the selected first set of communication scenario information and the current channel estimation wherein the current channel estimation is used to generate the channel quality indicator for each of the plurality of UE communication scenarios wherein the UE device is configured to transmit the channel quality indicator to base station The UE device of claim wherein each of the stored plurality of sets of communication scenario information is based on receiver type multiple input multiple output MIMO scheme and amount of Doppler shift wherein said determining the current communication scenario of the UE comprises determining receiver type of the UE MIMO scheme used by the base station and the amount of Doppler shift experienced by the UE The UE device of claim wherein each of the stored plurality of sets of communication scenario information comprises one or more mapping tables for mapping spectral efficiency to the channel quality indicator The UE device of claim wherein each of the stored plurality of sets of communication scenario information comprises at least first mapping table for mapping signal to noise ratio to spectral efficiency and second mapping table for mapping spectral efficiency to the channel quality indicator method performed by wireless user equipment UE the method comprising determining current communication scenario of the UE during operation of the UE wherein determining the current communication scenario is based on two or more of receiver type MIMO scheme and amount of Doppler shift selecting first set of communication scenario information based on the determined current communication scenario of the UE wherein the first set of communication scenario information is selected from stored plurality of sets of communication scenario information wherein each of the stored plurality of sets of communication scenario information is based on the two or more of receiver type multiple input multiple output MIMO scheme and amount of Doppler shift wherein each set of communication scenario information corresponds to respective UE communication scenario determining current channel estimation for the UE generating at least one channel quality indicator based on the selected first set of communication scenario information and based on the current channel estimation for the UE wherein the current channel estimation is used to generate the channel quality indicator for each of the communication scenarios and transmitting the channel quality indicator to base station The method of claim wherein each of the stored plurality of sets of communication scenario information is based on receiver type multiple input multiple output MIMO scheme and amount of Doppler shift wherein said determining the current communication scenario of the UE comprises determining receiver type of the UE MIMO scheme used by the base station and the amount of Doppler shift experienced by the UE The method of claim wherein each of the stored plurality of sets of communication scenario information comprises one or more mapping tables for mapping spectral efficiency to the channel quality indicator The method of claim wherein each of the stored plurality of sets of communication scenario information comprises at least first mapping table for mapping signal to noise ratio to spectral efficiency and second mapping table for mapping spectral efficiency to the channel quality indicator method for operating wireless user equipment UE the method comprising storing plurality of sets of communication scenario information for each of plurality of UE communication scenarios wherein each of the stored plurality of sets of communication scenario information is based on two or more of receiver type multiple input multiple output MIMO scheme and amount of Doppler shift wherein each set of communication scenario information is useable in generating channel quality indicator for respective communication scenario determining current communication scenario of the UE during operation of the UE wherein determining the current communication scenario is based on the two or more of receiver type MIMO scheme and amount of Doppler shift selecting first set of communication scenario information from the plurality of sets of communication scenario information based on the determined current communication scenario of the UE determining current channel estimation for the UE device generating at least one channel quality indicator based on the selected first set of communication scenario information and the current channel estimation wherein the current channel estimation is used to generate the channel quality indicator for each of the plurality of UE communication scenarios and providing the channel quality indicator to base station The method of claim wherein each of the stored plurality of sets of communication scenario information is based on receiver type multiple input multiple output MIMO scheme and amount of Doppler shift wherein said determining the current communication scenario of the UE comprises determining receiver type of the UE MIMO scheme used by the base station and the amount of Doppler shift experienced by the UE The method of claim wherein each of the stored plurality of sets of communication scenario information comprises one or more mapping tables for mapping spectral efficiency to the channel quality indicator The method of claim wherein each of the stored plurality of sets of communication scenario information comprises at least first mapping table for mapping signal to noise ratio to spectral efficiency and second mapping table for mapping spectral efficiency to the channel quality indicator non transitory computer accessible memory medium storing program instructions wherein the program instructions when executed by computer system cause the computer system to determine current communication scenario of the UE during operation of the UE wherein determining the current communication scenario is based on two or more of receiver type MIMO scheme and amount of Doppler shift select first set of communication scenario information based on the determined current communication scenario of the UE wherein the first set of communication scenario information is selected from stored plurality of sets of communication scenario information wherein each of the stored plurality of sets of communication scenario information is based on the two or more of receiver type multiple input multiple output MIMO scheme and amount of Doppler shift wherein each set of communication scenario information corresponds to respective UE communication scenario determine current channel estimation for the UE and generate at least one channel quality indicator based on the selected first set of communication scenario information and based on the current channel estimation for the UE wherein the current channel estimation is used to generate the channel quality indicator for each of communication scenarios wherein the channel quality indicator is configured to be transmitted to base station The non transitory computer accessible memory medium of claim wherein each of the stored plurality of sets of communication scenario information is based on receiver type multiple input multiple output MIMO scheme and amount of Doppler shift The non transitory computer accessible memory medium of claim wherein each of the stored plurality of sets of communication scenario information comprises one or more mapping tables for mapping spectral efficiency to the channel quality indicator method for configuring wireless user equipment UE the method comprising performing channel quality indicator adaptation method for each of plurality of UE communication scenarios to determine plurality of sets of communication scenario information wherein each of the stored plurality of sets of communication scenario information is based on two or more of receiver type multiple input multiple output MIMO scheme and amount of Doppler shift wherein each set of communication scenario information corresponds to one of the UE communication scenarios storing the plurality of sets of communication scenario information in the UE configuring the UE to selectively utilize ones of the plurality of sets of communication scenario information to generate channel quality indicator based on current communication conditions of the UE and based on current channel estimation for the UE wherein the current channel estimation is used to generate the channel quality indicator for each of the plurality of UE communication scenarios The method of claim wherein said performing the channel quality indicator adaptation method for each of the plurality of UE communication scenarios comprises selecting signal to noise ratio value defining spectral efficiency value based on the signal to noise ratio determining modulation and coding scheme based on the signal to noise ratio generating channel quality indicator value based on the modulation and coding scheme and the selected signal to noise ratio value performing d plurality of times to generate mapping table which maps spectral efficiency values to channel quality indicator values wherein the mapping table comprises the communication scenario information The method of claim wherein each of the stored plurality of sets of communication scenario information is based on receiver type multiple input multiple output MIMO scheme and amount of Doppler shift The method of claim wherein said configuring the UE comprises storing software program in memory of the UE which is executable by the processor The UE device of claim wherein the current communication scenario is based on the receiver type wherein the receiver types comprise one or more of Linear Minimum Mean Square Error LMMSE Maximum Likelihood Method MLM or Linear Minimum Mean Square Error with Serial Interference Cancellation LMMSE SIC The method of claim wherein the current communication scenario is based on the receiver type wherein the receiver types comprise one or more of Linear Minimum Mean Square Error LMMSE Maximum Likelihood Method MLM or Linear Minimum Mean Square Error with Serial Interference Cancellation LMMSE SIC The method of claim wherein the current communication scenario is based on the receiver type wherein the receiver types comprise one or more of Linear Minimum Mean Square Error LMMSE Maximum Likelihood Method MLM or Linear Minimum Mean Square Error with Serial Interference Cancellation LMMSE SIC The non transitory computer accessible memory medium of claim wherein the current communication scenario is based on the receiver type wherein the receiver types comprise one or more of Linear Minimum Mean Square Error LMMSE Maximum Likelihood Method MLM or Linear Minimum Mean Square Error with Serial Interference Cancellation LMMSE SIC The method of claim wherein the communication scenarios are based on the receiver type wherein the receiver types comprise one or more of Linear Minimum Mean Square Error LMMSE Maximum Likelihood Method MLM or Linear Minimum Mean Square Error with Serial Interference Cancellation LMMSE SIC '"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BbsphJkJHfRa",
    "outputId": "6354e03d-7fd0-493e-b799-4ebc59957cb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_labels = df.quality_rank.values\n",
    "#data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "xZNPH2hDHfRj",
    "outputId": "e1328a40-f64d-4092-e22a-2f55746e8b00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare for data label\n",
    "data_labels = to_categorical(df.quality_rank.values)\n",
    "data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FPWJTvdZHfRl"
   },
   "outputs": [],
   "source": [
    "# BERT tokenizer\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "svJ29kVWHfRn"
   },
   "outputs": [],
   "source": [
    "def encode_sentence(sent, max_seq_length):\n",
    "    if len(sent) <= max_seq_length:\n",
    "        return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]\n",
    "    else: # BERT limited to 512 tokens\n",
    "        return [\"[CLS]\"] + tokenizer.tokenize(sent)[-max_seq_length:] + [\"[SEP]\"] # change to last n words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "N0vEQFuPHfRp",
    "outputId": "8f0a411a-c0bd-46ba-8a81-96828818263d"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-eacb0cabf562>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m510\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenized_claims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclaims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-eacb0cabf562>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m510\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenized_claims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclaims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-62e4248c1c66>\u001b[0m in \u001b[0;36mencode_sentence\u001b[0;34m(sent, max_seq_length)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"[CLS]\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"[SEP]\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# BERT limited to 512 tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"[CLS]\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"[SEP]\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# change to last n words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bert/tokenization/bert_tokenization.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msub_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bert/tokenization/bert_tokenization.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;34m\"\"\"Tokenizes a piece of text.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bert/tokenization/bert_tokenization.py\u001b[0m in \u001b[0;36m_clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0xfffd\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0m_is_whitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# BERT takes maximum 512 sequence\n",
    "\n",
    "MAX_SEQ_LEN=510\n",
    "\n",
    "tokenized_claims = [encode_sentence(sentence, MAX_SEQ_LEN) for sentence in claims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWNmCF5YHfRr"
   },
   "outputs": [],
   "source": [
    "tokenized_claims[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "onTyweHhHfRt"
   },
   "outputs": [],
   "source": [
    "np.array(tokenized_claims).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__hCQmyJHfRv"
   },
   "outputs": [],
   "source": [
    "# Prepre the 3 inputs required by BERT deriving from the original data\n",
    "\n",
    "def get_ids(tokens):\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def get_mask(tokens):\n",
    "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)  # if not equal [PAD] then assign 1. \n",
    "\n",
    "def get_segments(tokens):\n",
    "    seg_ids = []\n",
    "    current_seg_id = 0\n",
    "    for tok in tokens:\n",
    "        seg_ids.append(current_seg_id)\n",
    "        if tok == \"[SEP]\":\n",
    "            current_seg_id = 1-current_seg_id # turns 1 into 0 and vice versa\n",
    "    return seg_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aycKAso7HfRx"
   },
   "source": [
    "Below to create padded batches (so there could have different sentence length between batches, but will be same sentence length within batch) this is to save processing memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yg-vylebHfRx"
   },
   "outputs": [],
   "source": [
    "data_with_len = [[sent, data_labels[i], len(sent)]\n",
    "                 for i, sent in enumerate(tokenized_claims)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upW7oXhBHfRz"
   },
   "outputs": [],
   "source": [
    "# split to train and test\n",
    "\n",
    "data_with_len_train = data_with_len[:35000]\n",
    "data_with_len_dev = data_with_len[35000:45000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5IKYLuLuHfR1"
   },
   "outputs": [],
   "source": [
    "data_with_len_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oYKVmEJ9HfR4"
   },
   "outputs": [],
   "source": [
    "np.array(data_with_len_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkcR-oooHfR5"
   },
   "outputs": [],
   "source": [
    "np.unique(np.array(data_with_len_train)[:, 2], return_counts = True)\n",
    "# majority are in 512 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uv0of9OcHfR7"
   },
   "outputs": [],
   "source": [
    "def to_batch(data_with_len, BATCH_SIZE):\n",
    "    data_with_len = sorted(data_with_len, key=lambda x: x[2])\n",
    "    sorted_all = [([get_ids(sent_lab[0]),\n",
    "                get_mask(sent_lab[0]),\n",
    "                get_segments(sent_lab[0])],\n",
    "               sent_lab[1])\n",
    "              for sent_lab in data_with_len] \n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
    "                                             output_types=(tf.int32, tf.int32))\n",
    "    \n",
    "\n",
    "    train_batched = train_dataset.padded_batch(BATCH_SIZE, # this is the pre-processed input to feed into BERT embedding layer and DCNN model\n",
    "                                       padded_shapes=((3, 512), [2]), # oh, here is the key, need to change from () to [2] now that I pass in the one-hot label!\n",
    "                                       padding_values=(0, 0)) # drop_remainder = True (optional)\n",
    "    return train_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Wi8NsdJHfR9"
   },
   "outputs": [],
   "source": [
    "# prepare the batches for train set:\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_batched = to_batch(data_with_len_train, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MQj-CVGAHfR_",
    "outputId": "a8944973-382f-4f44-ec30-c3a3a6de3b10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PaddedBatchDataset shapes: ((None, 3, 512), (None, 2)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batched # note: if above use drop_remainder = True, then => shapes: ((32, 3, None), (32, 2)), types: (tf.int32, tf.int32)>  because it would only be \"None\" if any single item is inconsistent in shape with the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9F0D2NpPHfSC",
    "outputId": "035c82f7-0f85-483e-82db-3c62f16ffdb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 3, 512), dtype=int32, numpy=\n",
       " array([[[  101,  7328,  2383, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2054,  2003, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2057,  4366, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  101,  1996, 11028, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2054,  2003, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2054,  2003, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(32, 2), dtype=int32, numpy=\n",
       " array([[0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0]], dtype=int32)>)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_batched))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "osgD-5o5HfSD"
   },
   "outputs": [],
   "source": [
    "# prepare the batches for evaluation set:\n",
    "test_batched = to_batch(data_with_len_dev, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GQEILGogHfSF",
    "outputId": "99ba0471-7b6d-4563-b282-19d01bc97774"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 3, 512), dtype=int32, numpy=\n",
       " array([[[  101,  1996, 11028, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2054,  2003, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  1996, 11028, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  101,  1996, 11028, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2057,  4366, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  4118,  1997, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(32, 2), dtype=int32, numpy=\n",
       " array([[1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0]], dtype=int32)>)"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_batched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "wYuTEUg2HfSH",
    "outputId": "c8e3b29a-aff5-4119-df7a-da49d42bab4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Count:10000, Positive Label Count:[5898 4102], Positive Class Ratio:[0.5898 0.4102], Negative Class Ratio = [0.4102 0.5898]\n"
     ]
    }
   ],
   "source": [
    "# double check the label distribution\n",
    "# Validation set\n",
    "count = 0\n",
    "sum_positive = 0\n",
    "for element in test_batched:\n",
    "    count += len(element[1])\n",
    "    sum_positive += sum(element[1])\n",
    "\n",
    "print(f'Sample Count:{count}, Positive Label Count:{sum_positive}, Positive Class Ratio:{sum_positive/count}, Negative Class Ratio = {1-sum_positive/count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "U0UuHXcmHfSK",
    "outputId": "7a45a6c3-15ba-4ada-e789-b023f75dbc45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Count:35000, Positive Label Count:[20815 14185], Positive Class Ratio:[0.59471429 0.40528571], Negative Class Ratio = [0.40528571 0.59471429]\n"
     ]
    }
   ],
   "source": [
    "# double check the label distribution\n",
    "# Train set\n",
    "count = 0\n",
    "sum_positive = 0\n",
    "for element in train_batched:\n",
    "    count += len(element[1])\n",
    "    sum_positive += sum(element[1])\n",
    "\n",
    "print(f'Sample Count:{count}, Positive Label Count:{sum_positive}, Positive Class Ratio:{sum_positive/count}, Negative Class Ratio = {1-sum_positive/count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EgY2cWX9HfSM"
   },
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gzhC87nDHfSM"
   },
   "outputs": [],
   "source": [
    "class DCNNBERTEmbedding(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_filters=50,\n",
    "                 FFN_units=512,\n",
    "                 nb_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 name=\"dcnn\"):\n",
    "        super(DCNNBERTEmbedding, self).__init__(name=name)\n",
    "        \n",
    "        self.bert_layer = hub.KerasLayer(\n",
    "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "            trainable=False)\n",
    "\n",
    "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
    "                                    kernel_size=5,\n",
    "                                    padding=\"valid\",\n",
    "                                    activation=\"relu\")\n",
    "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
    "                                     kernel_size=50,\n",
    "                                     padding=\"valid\",\n",
    "                                     activation=\"relu\")\n",
    "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
    "                                      kernel_size=100,\n",
    "                                      padding=\"valid\",\n",
    "                                      activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if nb_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=2,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=nb_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def embed_with_bert(self, all_tokens):\n",
    "        _, embs = self.bert_layer([all_tokens[:, 0, :],\n",
    "                                   all_tokens[:, 1, :],\n",
    "                                   all_tokens[:, 2, :]])\n",
    "        return embs\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.embed_with_bert(inputs)\n",
    "\n",
    "        x_1 = self.bigram(x)\n",
    "        x_1 = self.pool(x_1)\n",
    "        x_2 = self.trigram(x)\n",
    "        x_2 = self.pool(x_2)\n",
    "        x_3 = self.fourgram(x)\n",
    "        x_3 = self.pool(x_3)\n",
    "        \n",
    "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HIJFKWzgHfSO"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jelk_-6HHfSP"
   },
   "outputs": [],
   "source": [
    "NB_FILTERS = 100\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = 2\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "# BATCH_SIZE = 32  # initialize above\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cmqwi2e2HfSR"
   },
   "outputs": [],
   "source": [
    "Dcnn = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n",
    "                         FFN_units=FFN_UNITS,\n",
    "                         nb_classes=NB_CLASSES,\n",
    "                         dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzl_np4uHfST"
   },
   "outputs": [],
   "source": [
    "if NB_CLASSES == 2:\n",
    "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "else:\n",
    "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rFRtls_STNEg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3jPRw3XiHfSV",
    "outputId": "c9771dd6-cff9-4da7-8ccb-ed9f5a27eebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"/content/drive/My Drive/NLP_Udemy/data/ckpt_BERT_CNN_US_5years_last_510_words_50k/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPGxQ8XwHfSX"
   },
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ckpt_manager.save()\n",
    "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "TqcqbJqLHfSZ",
    "outputId": "e9b807e6-e0b5-437b-ad53-570769de79bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   1094/Unknown - 1760s 2s/step - loss: 0.6734 - accuracy: 0.6169Checkpoint saved at /content/drive/My Drive/NLP_Udemy/data/ckpt_BERT_CNN_US_5years_last_510_words_50k/.\n",
      "1094/1094 [==============================] - 2237s 2s/step - loss: 0.6734 - accuracy: 0.6169 - val_loss: 0.6609 - val_accuracy: 0.6064\n",
      "Epoch 2/5\n",
      "1094/1094 [==============================] - ETA: 0s - loss: 0.6296 - accuracy: 0.6454Checkpoint saved at /content/drive/My Drive/NLP_Udemy/data/ckpt_BERT_CNN_US_5years_last_510_words_50k/.\n",
      "1094/1094 [==============================] - 2236s 2s/step - loss: 0.6296 - accuracy: 0.6454 - val_loss: 0.6518 - val_accuracy: 0.6148\n",
      "Epoch 3/5\n",
      "1094/1094 [==============================] - ETA: 0s - loss: 0.6118 - accuracy: 0.6600Checkpoint saved at /content/drive/My Drive/NLP_Udemy/data/ckpt_BERT_CNN_US_5years_last_510_words_50k/.\n",
      "1094/1094 [==============================] - 2235s 2s/step - loss: 0.6118 - accuracy: 0.6600 - val_loss: 0.6597 - val_accuracy: 0.6030\n",
      "Epoch 4/5\n",
      "1094/1094 [==============================] - ETA: 0s - loss: 0.5919 - accuracy: 0.6788Checkpoint saved at /content/drive/My Drive/NLP_Udemy/data/ckpt_BERT_CNN_US_5years_last_510_words_50k/.\n",
      "1094/1094 [==============================] - 2235s 2s/step - loss: 0.5919 - accuracy: 0.6788 - val_loss: 0.6733 - val_accuracy: 0.6034\n",
      "Epoch 5/5\n",
      "1094/1094 [==============================] - ETA: 0s - loss: 0.5679 - accuracy: 0.7022Checkpoint saved at /content/drive/My Drive/NLP_Udemy/data/ckpt_BERT_CNN_US_5years_last_510_words_50k/.\n",
      "1094/1094 [==============================] - 2240s 2s/step - loss: 0.5679 - accuracy: 0.7022 - val_loss: 0.6928 - val_accuracy: 0.6057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6091c9c518>"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting time\n",
    "Dcnn.fit(train_batched,\n",
    "         validation_data = (test_batched),\n",
    "         epochs=NB_EPOCHS,\n",
    "         callbacks=[MyCustomCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWF1APgnHfSb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XlxF-BewHfSp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SEUeI7EkHfSr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b12s2KKVHfSu"
   },
   "outputs": [],
   "source": [
    "# modify from original to_batch in order to not sort to different order because for ensemble we need to preserve the same order among all models\n",
    "def prediction_to_batch(data_with_len, BATCH_SIZE):\n",
    "    # data_with_len = sorted(data_with_len, key=lambda x: x[2])\n",
    "    sorted_all = [([get_ids(sent_lab[0]),\n",
    "                get_mask(sent_lab[0]),\n",
    "                get_segments(sent_lab[0])],\n",
    "               sent_lab[1])\n",
    "              for sent_lab in data_with_len] \n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
    "                                             output_types=(tf.int32, tf.int32))\n",
    "    \n",
    "\n",
    "    train_batched = train_dataset.padded_batch(BATCH_SIZE, # this is the pre-processed input to feed into BERT embedding layer and DCNN model\n",
    "                                       padded_shapes=((3, 512), [2]), # make from None to 512 fixed dimension\n",
    "                                       padding_values=(0, 0)) # drop_remainder = True (optional)\n",
    "    return train_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "738jNq6yHfSw"
   },
   "outputs": [],
   "source": [
    "# prepare the batches for evaluation set:\n",
    "test_batched_prediction = prediction_to_batch(data_with_len_dev, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LloS9bPxHfSy",
    "outputId": "44bce9ed-2972-440c-e8dc-19706c4279fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 3, 512), dtype=int32, numpy=\n",
       " array([[[  101,  1997,  1996, ...,  5492,  6943,   102],\n",
       "         [    1,     1,     1, ...,     1,     1,     1],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2054,  2003, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2000,  2055, ...,  8681, 16216,   102],\n",
       "         [    1,     1,     1, ...,     1,     1,     1],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  101,  6058,  3538, ...,  7277,  3430,   102],\n",
       "         [    1,     1,     1, ...,     1,     1,     1],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  4860, 13907, ..., 17785, 15273,   102],\n",
       "         [    1,     1,     1, ...,     1,     1,     1],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101, 15176, 10984, ...,  2140,  4984,   102],\n",
       "         [    1,     1,     1, ...,     1,     1,     1],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(32, 2), dtype=int32, numpy=\n",
       " array([[1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1]], dtype=int32)>)"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_batched_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Jdsjq6x4HfS0",
    "outputId": "bad75a24-e524-46ef-eda1-c2fe9febe4bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5658154 , 0.4443321 ],\n",
       "       [0.581872  , 0.4347374 ],\n",
       "       [0.36238575, 0.6277524 ],\n",
       "       ...,\n",
       "       [0.6997249 , 0.29793364],\n",
       "       [0.96749085, 0.02547945],\n",
       "       [0.38488117, 0.6237045 ]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test = Dcnn.predict(test_batched_prediction)\n",
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qVfkbggRHfS3",
    "outputId": "d8be7b37-6a42-4142-87e8-7bd452ee4f4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "24vxr6ctPole",
    "outputId": "e94fdd40-2516-4773-ca95-3d37b312084b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iLfEdKVDHfS5"
   },
   "outputs": [],
   "source": [
    "np.savetxt('/content/drive/My Drive/NLP_Udemy/data/BERT_CNN_5yr_claims_last_510_words_50k_dev_prob.csv', pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3lwi7joKHfS6"
   },
   "outputs": [],
   "source": [
    "predicted = [np.argmax(pred) for pred in \n",
    "             pred_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvRt6VsfHfS9"
   },
   "outputs": [],
   "source": [
    "y_test_binary = df['quality_rank'][35000:45000].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "xDh2GobUHfS-",
    "outputId": "6f52dcd9-5a98-483a-8782-917592cb7dad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.606\n",
      "Auc: 0.602\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.62      0.65      5898\n",
      "           1       0.52      0.58      0.55      4102\n",
      "\n",
      "    accuracy                           0.61     10000\n",
      "   macro avg       0.60      0.60      0.60     10000\n",
      "weighted avg       0.61      0.61      0.61     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test_binary, predicted)\n",
    "auc = metrics.roc_auc_score(y_test_binary, predicted)  # predicted_prob), check doc, seems the second argument required to be shape (n_samples,) for binary case \n",
    "                            #multi_class=\"ovr\") # check documentation and seems \"ovr\" not good for only binary target class\n",
    "print(\"Accuracy:\",  round(accuracy,3))\n",
    "print(\"Auc:\", round(auc,3))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test_binary, predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FxOOIZdjDBA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT+CNN_US_index4_5years_claims_last_510_words_50k-Copy1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
