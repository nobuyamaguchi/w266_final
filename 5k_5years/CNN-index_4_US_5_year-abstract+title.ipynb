{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# string processing\n",
    "import re\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing, feature_selection, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to navigate to the data location\n",
    "import os\n",
    "\n",
    "# get current directory \n",
    "path = os.getcwd() \n",
    "\n",
    "# parent directory\n",
    "parent = os.path.dirname(path)\n",
    "\n",
    "df_merge_quality = pd.read_csv(parent + '/data/US_patent_abstract_5000_2015_with_title_1_5y.csv')\n",
    "df_merge_quality.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>quality_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Invitation information push method and system....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coronal angulating connector. A connector is p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spearfishing apparatus. A device for spearfish...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Systems and methods for prioritizing media fil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Semiconductor integrated circuit. A semiconduc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Cross-platform cloud-based map creation. Metho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Display substrate. A display substrate include...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Aminoquinazoline derivatives and their salts a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Method and device for displaying information i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>System and method for ultrasonic metering usin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  quality_rank\n",
       "0     Invitation information push method and system....             0\n",
       "1     Coronal angulating connector. A connector is p...             0\n",
       "2     Spearfishing apparatus. A device for spearfish...             1\n",
       "3     Systems and methods for prioritizing media fil...             1\n",
       "4     Semiconductor integrated circuit. A semiconduc...             0\n",
       "...                                                 ...           ...\n",
       "4995  Cross-platform cloud-based map creation. Metho...             1\n",
       "4996  Display substrate. A display substrate include...             1\n",
       "4997  Aminoquinazoline derivatives and their salts a...             1\n",
       "4998  Method and device for displaying information i...             1\n",
       "4999  System and method for ultrasonic metering usin...             1\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_merge_quality[['text', 'quality_rank']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Invitation information push method and system. An invitation information push method includes after receiving an invitation request sent by a microblog user, a server sending invitation information to a number of clients corresponding to invited users carried in the invitation request, wherein the invited users are users who have not registered microblog, and the number of the invited users N is greater than or equal to 1. Each client, upon receiving the invitation information, creating an invitation information guide to guide the users who have not registered the microblog to register the microblog. The method further comprises, when a predetermined time is reached, a server actively sending invitation information to at least one client corresponding to at least one user who has not registered the microblog.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lin_menghsien/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more CNN library\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1500  # [Steven] I added for the CNN, to take only the 500 words.\n",
    "# MAX_SENT_LENGTH = 100\n",
    "# MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 30000  # [Steven] this should be number of unique word / vocabulary\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", str(string))    \n",
    "    string = re.sub(r\"\\'\", \"\", str(string))    \n",
    "    string = re.sub(r\"\\\"\", \"\", str(string))    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>quality_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Invitation information push method and system....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coronal angulating connector. A connector is p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spearfishing apparatus. A device for spearfish...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Systems and methods for prioritizing media fil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Semiconductor integrated circuit. A semiconduc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Cross-platform cloud-based map creation. Metho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Display substrate. A display substrate include...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Aminoquinazoline derivatives and their salts a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Method and device for displaying information i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>System and method for ultrasonic metering usin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  quality_rank\n",
       "0     Invitation information push method and system....             0\n",
       "1     Coronal angulating connector. A connector is p...             0\n",
       "2     Spearfishing apparatus. A device for spearfish...             1\n",
       "3     Systems and methods for prioritizing media fil...             1\n",
       "4     Semiconductor integrated circuit. A semiconduc...             0\n",
       "...                                                 ...           ...\n",
       "4995  Cross-platform cloud-based map creation. Metho...             1\n",
       "4996  Display substrate. A display substrate include...             1\n",
       "4997  Aminoquinazoline derivatives and their salts a...             1\n",
       "4998  Method and device for displaying information i...             1\n",
       "4999  System and method for ultrasonic metering usin...             1\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_train = df\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "abstracts = [] # abstracts is list of list of list to hold each sentences of each abstract (the most complete data)\n",
    "labels = [] # label is just a list holding our label which is quality_index\n",
    "texts = []  # texts to hold each complete abstract as list of list (note: abstract not breaking up to sentence level)\n",
    "for idx in range(data_train.text.shape[0]): # for each row\n",
    "    #text = BeautifulSoup(data_train.text[idx])\n",
    "    #print(clean_str(str(data_train.iloc[idx]['text'])))\n",
    "    text = clean_str(str(data_train.iloc[idx]['text'])) # text is each complete abstract\n",
    "    texts.append(text) # texts to hold each complete abstract as list of string (note: abstract not breaking up to sentence level)\n",
    "    sentences = tokenize.sent_tokenize(text) # sentences is list of string holding each complete sentence of one abstract (but it's just an intermediate variable, not used directly in later code)\n",
    "    abstracts.append(sentences) # abstracts is list of list of string to hold each sentences of each abstract (the most complete data) (this is what we use )\n",
    "    labels.append(data_train.iloc[idx]['quality_rank']) # label is just a list holding our label which is quality_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS) # intend to use next line .fit_on_texts to index each word within specific abstract at current iteration/loop, the more frequent word has lower index number, it is a dictionary format, it's like a unique vocabulary index\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "# data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16864 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3894,    45,  2436, ...,     0,     0,     0],\n",
       "       [ 9311, 11718,   278, ...,     0,     0,     0],\n",
       "       [ 9312,    31,     2, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [ 9310,  1518,     4, ...,     0,     0,     0],\n",
       "       [   15,     4,    12, ...,     0,     0,     0],\n",
       "       [   24,     4,    15, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create array of equal length to feed into model\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post') # The default is pre-padding, should I try post-padding ? => padding='post', truncating='post'. I think it has to do with whether the beginning words are more important, or later words more important. You may want to try both approaches and compare result?\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1500)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (5000, 1500)\n",
      "Shape of label tensor: (5000, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "num_validation_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set\n",
      "[2397. 1603.]\n",
      "[595. 405.]\n"
     ]
    }
   ],
   "source": [
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to navigate to the data location\n",
    "import os\n",
    "\n",
    "# get current directory \n",
    "path = os.getcwd() \n",
    "\n",
    "# parent directory\n",
    "parent = os.path.dirname(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import GloVe word embedding\n",
    "GLOVE_DIR = \"\" \n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(parent, GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16865"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  # EMBEDDING_DIM = 100 since we import 100d GloVe\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json, time, datetime, shutil\n",
    "from numpy.random import seed\n",
    "seed(5)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parameter setting\n",
    "epochs = 10\n",
    "embed_dim = EMBEDDING_DIM  # already specify with EMBEDDING_DIM variable above\n",
    "dense_layer_dims = [100]\n",
    "dropout_rate = 0.2\n",
    "num_filters = [25, 25, 50, 50, 50]\n",
    "kernel_sizes = [3, 4, 15, 50, 150]\n",
    "num_classes = labels.shape[1] # \n",
    "\n",
    "# Model building\n",
    "start_time = time.time()\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "h = embedding_layer(sequence_input) # remember embedding_layer assign with GloVe's weight\n",
    "\n",
    "conv_layers_for_all_kernel_sizes = []\n",
    "counter = 0\n",
    "for kernel_size, filters in zip(kernel_sizes, num_filters): \n",
    "    conv_layer = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(h)\n",
    "    conv_layer = keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers_for_all_kernel_sizes.append(conv_layer)\n",
    "    counter += 1\n",
    "    #print(conv_layers_for_all_kernel_sizes)\n",
    "if counter == 1: # need this since layers.concatenate throw error when there is only 1 item in conv_layers_for_all_kernel_sizes\n",
    "    h = conv_layer\n",
    "else:\n",
    "    h = keras.layers.concatenate(conv_layers_for_all_kernel_sizes, axis=1) # Concat the feature maps from each different size.\n",
    "\n",
    "    h = keras.layers.Dropout(rate=dropout_rate)(h) # Dropout can help with overfitting (improve generalization) by randomly 0-ing different subsets of values in the vector\n",
    "\n",
    "for dim in dense_layer_dims:\n",
    "    h = keras.layers.Dense(dim, activation = 'relu')(h) # [Steven] I believe this add additional fully-connected hidden layer with the hope to increase model accuracy\n",
    "\n",
    "prediction = keras.layers.Dense(num_classes, activation='sigmoid')(h)\n",
    "\n",
    "model = keras.Model(inputs=sequence_input, outputs=prediction)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # From information theory notebooks.\n",
    "              metrics=['accuracy'])        # What metric to output as we train.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "# setup checkpoint\n",
    "\n",
    "checkpoint_path = \"ckpt_CNN_US_5years/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(model = model) \n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ckpt_manager.save()\n",
    "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.7116 - accuracy: 0.5775Checkpoint saved at ckpt_CNN_US_5years/.\n",
      "125/125 [==============================] - 338s 3s/step - loss: 0.7116 - accuracy: 0.5775 - val_loss: 0.6690 - val_accuracy: 0.6060\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6360 - accuracy: 0.6438Checkpoint saved at ckpt_CNN_US_5years/.\n",
      "125/125 [==============================] - 348s 3s/step - loss: 0.6360 - accuracy: 0.6438 - val_loss: 0.7127 - val_accuracy: 0.5980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.5396 - accuracy: 0.7390Checkpoint saved at ckpt_CNN_US_5years/.\n",
      "125/125 [==============================] - 338s 3s/step - loss: 0.5396 - accuracy: 0.7390 - val_loss: 0.6841 - val_accuracy: 0.6240\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.3701 - accuracy: 0.8410Checkpoint saved at ckpt_CNN_US_5years/.\n",
      "125/125 [==============================] - 344s 3s/step - loss: 0.3701 - accuracy: 0.8410 - val_loss: 0.8630 - val_accuracy: 0.5380\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1777 - accuracy: 0.9392Checkpoint saved at ckpt_CNN_US_5years/.\n",
      "125/125 [==============================] - 342s 3s/step - loss: 0.1777 - accuracy: 0.9392 - val_loss: 0.9896 - val_accuracy: 0.5090\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9643Checkpoint saved at ckpt_CNN_US_5years/.\n",
      "125/125 [==============================] - 344s 3s/step - loss: 0.1075 - accuracy: 0.9643 - val_loss: 0.9543 - val_accuracy: 0.5720\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9898Checkpoint saved at ckpt_CNN_US_5years/.\n",
      "125/125 [==============================] - 352s 3s/step - loss: 0.0525 - accuracy: 0.9898 - val_loss: 1.0647 - val_accuracy: 0.5830\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9945Checkpoint saved at ckpt_CNN_US_5years/.\n",
      "125/125 [==============================] - 350s 3s/step - loss: 0.0297 - accuracy: 0.9945 - val_loss: 1.1443 - val_accuracy: 0.5860\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9915Checkpoint saved at ckpt_CNN_US_5years/.\n",
      "125/125 [==============================] - 350s 3s/step - loss: 0.0356 - accuracy: 0.9915 - val_loss: 1.1247 - val_accuracy: 0.5890\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9895Checkpoint saved at ckpt_CNN_US_5years/.\n",
      "125/125 [==============================] - 345s 3s/step - loss: 0.0363 - accuracy: 0.9895 - val_loss: 1.3113 - val_accuracy: 0.5410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f707bdd5550>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "model.reset_states()\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          epochs=epochs, \n",
    "          validation_data=(x_val, y_val),\n",
    "         callbacks=[MyCustomCallback()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "# restore epoch 9\n",
    "\n",
    "checkpoint_path = \"ckpt_CNN_US_5years/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(model = model) \n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore('ckpt_CNN_US_5years/ckpt-9')\n",
    "    print(\"Latest checkpoint restored!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73568666, 0.26373106],\n",
       "       [0.31567356, 0.6768236 ],\n",
       "       [0.21639621, 0.81224877],\n",
       "       ...,\n",
       "       [0.00127256, 0.99828047],\n",
       "       [0.2456066 , 0.7296507 ],\n",
       "       [0.6932094 , 0.3666255 ]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test = model.predict(x_val)\n",
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Predict_Output/CNN_5yr_abstract_title_dev_prob.csv', pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = [np.argmax(pred) for pred in \n",
    "             pred_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_binary = df['quality_rank'][4000:].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.589\n",
      "Auc: 0.545\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.77      0.69       595\n",
      "           1       0.49      0.32      0.38       405\n",
      "\n",
      "    accuracy                           0.59      1000\n",
      "   macro avg       0.56      0.55      0.54      1000\n",
      "weighted avg       0.57      0.59      0.57      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test_binary, predicted)\n",
    "auc = metrics.roc_auc_score(y_test_binary, predicted)  # predicted_prob), check doc, seems the second argument required to be shape (n_samples,) for binary case \n",
    "                            #multi_class=\"ovr\") # check documentation and seems \"ovr\" not good for only binary target class\n",
    "print(\"Accuracy:\",  round(accuracy,3))\n",
    "print(\"Auc:\", round(auc,3))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test_binary, predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
