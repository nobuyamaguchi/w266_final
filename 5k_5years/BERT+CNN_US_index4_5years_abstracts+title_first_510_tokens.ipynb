{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as up\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "import bert\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "# string processing\n",
    "import re\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing, feature_selection, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to navigate to the data location\n",
    "import os\n",
    "\n",
    "# get current directory \n",
    "path = os.getcwd() \n",
    "\n",
    "# parent directory\n",
    "parent = os.path.dirname(path)\n",
    "\n",
    "df_merge_quality = pd.read_csv(parent + '/data/US_patent_abstract_5000_2015_with_title_1_5y.csv')\n",
    "df_merge_quality.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>quality_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Invitation information push method and system....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coronal angulating connector. A connector is p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spearfishing apparatus. A device for spearfish...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Systems and methods for prioritizing media fil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Semiconductor integrated circuit. A semiconduc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Cross-platform cloud-based map creation. Metho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Display substrate. A display substrate include...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Aminoquinazoline derivatives and their salts a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Method and device for displaying information i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>System and method for ultrasonic metering usin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  quality_rank\n",
       "0     Invitation information push method and system....             0\n",
       "1     Coronal angulating connector. A connector is p...             0\n",
       "2     Spearfishing apparatus. A device for spearfish...             1\n",
       "3     Systems and methods for prioritizing media fil...             1\n",
       "4     Semiconductor integrated circuit. A semiconduc...             0\n",
       "...                                                 ...           ...\n",
       "4995  Cross-platform cloud-based map creation. Metho...             1\n",
       "4996  Display substrate. A display substrate include...             1\n",
       "4997  Aminoquinazoline derivatives and their salts a...             1\n",
       "4998  Method and device for displaying information i...             1\n",
       "4999  System and method for ultrasonic metering usin...             1\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_merge_quality[['text', 'quality_rank']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = []\n",
    "sentences = list(df['text'])\n",
    "for sen in sentences:\n",
    "    claims.append(preprocess_text(str(sen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Invitation information push method and system An invitation information push method includes after receiving an invitation request sent by microblog user server sending invitation information to number of clients corresponding to invited users carried in the invitation request wherein the invited users are users who have not registered microblog and the number of the invited users is greater than or equal to Each client upon receiving the invitation information creating an invitation information guide to guide the users who have not registered the microblog to register the microblog The method further comprises when predetermined time is reached server actively sending invitation information to at least one client corresponding to at least one user who has not registered the microblog '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare for data label\n",
    "data_labels = to_categorical(df.quality_rank.values)\n",
    "data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenizer\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sent, max_seq_length):\n",
    "    if len(sent) <= max_seq_length:\n",
    "        return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]\n",
    "    else: # BERT limited to 512 tokens\n",
    "        return [\"[CLS]\"] + tokenizer.tokenize(sent)[:max_seq_length] + [\"[SEP]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT takes maximum 512 sequence\n",
    "\n",
    "MAX_SEQ_LEN=510\n",
    "\n",
    "tokenized_claims = [encode_sentence(sentence, MAX_SEQ_LEN) for sentence in claims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[CLS]',\n",
       "  'invitation',\n",
       "  'information',\n",
       "  'push',\n",
       "  'method',\n",
       "  'and',\n",
       "  'system',\n",
       "  'an',\n",
       "  'invitation',\n",
       "  'information',\n",
       "  'push',\n",
       "  'method',\n",
       "  'includes',\n",
       "  'after',\n",
       "  'receiving',\n",
       "  'an',\n",
       "  'invitation',\n",
       "  'request',\n",
       "  'sent',\n",
       "  'by',\n",
       "  'micro',\n",
       "  '##bl',\n",
       "  '##og',\n",
       "  'user',\n",
       "  'server',\n",
       "  'sending',\n",
       "  'invitation',\n",
       "  'information',\n",
       "  'to',\n",
       "  'number',\n",
       "  'of',\n",
       "  'clients',\n",
       "  'corresponding',\n",
       "  'to',\n",
       "  'invited',\n",
       "  'users',\n",
       "  'carried',\n",
       "  'in',\n",
       "  'the',\n",
       "  'invitation',\n",
       "  'request',\n",
       "  'wherein',\n",
       "  'the',\n",
       "  'invited',\n",
       "  'users',\n",
       "  'are',\n",
       "  'users',\n",
       "  'who',\n",
       "  'have',\n",
       "  'not',\n",
       "  'registered',\n",
       "  'micro',\n",
       "  '##bl',\n",
       "  '##og',\n",
       "  'and',\n",
       "  'the',\n",
       "  'number',\n",
       "  'of',\n",
       "  'the',\n",
       "  'invited',\n",
       "  'users',\n",
       "  'is',\n",
       "  'greater',\n",
       "  'than',\n",
       "  'or',\n",
       "  'equal',\n",
       "  'to',\n",
       "  'each',\n",
       "  'client',\n",
       "  'upon',\n",
       "  'receiving',\n",
       "  'the',\n",
       "  'invitation',\n",
       "  'information',\n",
       "  'creating',\n",
       "  'an',\n",
       "  'invitation',\n",
       "  'information',\n",
       "  'guide',\n",
       "  'to',\n",
       "  'guide',\n",
       "  'the',\n",
       "  'users',\n",
       "  'who',\n",
       "  'have',\n",
       "  'not',\n",
       "  'registered',\n",
       "  'the',\n",
       "  'micro',\n",
       "  '##bl',\n",
       "  '##og',\n",
       "  'to',\n",
       "  'register',\n",
       "  'the',\n",
       "  'micro',\n",
       "  '##bl',\n",
       "  '##og',\n",
       "  'the',\n",
       "  'method',\n",
       "  'further',\n",
       "  'comprises',\n",
       "  'when',\n",
       "  'pre',\n",
       "  '##de',\n",
       "  '##ter',\n",
       "  '##mined',\n",
       "  'time',\n",
       "  'is',\n",
       "  'reached',\n",
       "  'server',\n",
       "  'actively',\n",
       "  'sending',\n",
       "  'invitation',\n",
       "  'information',\n",
       "  'to',\n",
       "  'at',\n",
       "  'least',\n",
       "  'one',\n",
       "  'client',\n",
       "  'corresponding',\n",
       "  'to',\n",
       "  'at',\n",
       "  'least',\n",
       "  'one',\n",
       "  'user',\n",
       "  'who',\n",
       "  'has',\n",
       "  'not',\n",
       "  'registered',\n",
       "  'the',\n",
       "  'micro',\n",
       "  '##bl',\n",
       "  '##og',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'corona',\n",
       "  '##l',\n",
       "  'ang',\n",
       "  '##ulating',\n",
       "  'connector',\n",
       "  'connector',\n",
       "  'is',\n",
       "  'provided',\n",
       "  'for',\n",
       "  'linear',\n",
       "  'implant',\n",
       "  '##s',\n",
       "  'such',\n",
       "  'as',\n",
       "  'spinal',\n",
       "  'rods',\n",
       "  'which',\n",
       "  'are',\n",
       "  'disposed',\n",
       "  'within',\n",
       "  'the',\n",
       "  'corona',\n",
       "  '##l',\n",
       "  'plane',\n",
       "  'of',\n",
       "  'body',\n",
       "  'the',\n",
       "  'connector',\n",
       "  'includes',\n",
       "  'first',\n",
       "  'portion',\n",
       "  'having',\n",
       "  'first',\n",
       "  'cavity',\n",
       "  'for',\n",
       "  'disposal',\n",
       "  'there',\n",
       "  '##th',\n",
       "  '##rou',\n",
       "  '##gh',\n",
       "  'of',\n",
       "  'first',\n",
       "  'spinal',\n",
       "  'rod',\n",
       "  'second',\n",
       "  'portion',\n",
       "  'has',\n",
       "  'second',\n",
       "  'cavity',\n",
       "  'for',\n",
       "  'the',\n",
       "  'disposal',\n",
       "  'there',\n",
       "  '##th',\n",
       "  '##rou',\n",
       "  '##gh',\n",
       "  'of',\n",
       "  'second',\n",
       "  'spinal',\n",
       "  'rod',\n",
       "  'the',\n",
       "  'second',\n",
       "  'portion',\n",
       "  'is',\n",
       "  'rot',\n",
       "  '##atable',\n",
       "  'relative',\n",
       "  'to',\n",
       "  'the',\n",
       "  'first',\n",
       "  'portion',\n",
       "  'methods',\n",
       "  'of',\n",
       "  'use',\n",
       "  'are',\n",
       "  'disclosed',\n",
       "  '[SEP]']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_claims[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tokenized_claims).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepre the 3 inputs required by BERT deriving from the original data\n",
    "\n",
    "def get_ids(tokens):\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def get_mask(tokens):\n",
    "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)  # if not equal [PAD] then assign 1. \n",
    "\n",
    "def get_segments(tokens):\n",
    "    seg_ids = []\n",
    "    current_seg_id = 0\n",
    "    for tok in tokens:\n",
    "        seg_ids.append(current_seg_id)\n",
    "        if tok == \"[SEP]\":\n",
    "            current_seg_id = 1-current_seg_id # turns 1 into 0 and vice versa\n",
    "    return seg_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below to create padded batches (so there could have different sentence length between batches, but will be same sentence length within batch) this is to save processing memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_len = [[sent, data_labels[i], len(sent)]\n",
    "                 for i, sent in enumerate(tokenized_claims)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to train and test\n",
    "\n",
    "data_with_len_train = data_with_len[:4000]\n",
    "data_with_len_test = data_with_len[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[CLS]',\n",
       "  'invitation',\n",
       "  'information',\n",
       "  'push',\n",
       "  'method',\n",
       "  'and',\n",
       "  'system',\n",
       "  'an',\n",
       "  'invitation',\n",
       "  'information',\n",
       "  'push',\n",
       "  'method',\n",
       "  'includes',\n",
       "  'after',\n",
       "  'receiving',\n",
       "  'an',\n",
       "  'invitation',\n",
       "  'request',\n",
       "  'sent',\n",
       "  'by',\n",
       "  'micro',\n",
       "  '##bl',\n",
       "  '##og',\n",
       "  'user',\n",
       "  'server',\n",
       "  'sending',\n",
       "  'invitation',\n",
       "  'information',\n",
       "  'to',\n",
       "  'number',\n",
       "  'of',\n",
       "  'clients',\n",
       "  'corresponding',\n",
       "  'to',\n",
       "  'invited',\n",
       "  'users',\n",
       "  'carried',\n",
       "  'in',\n",
       "  'the',\n",
       "  'invitation',\n",
       "  'request',\n",
       "  'wherein',\n",
       "  'the',\n",
       "  'invited',\n",
       "  'users',\n",
       "  'are',\n",
       "  'users',\n",
       "  'who',\n",
       "  'have',\n",
       "  'not',\n",
       "  'registered',\n",
       "  'micro',\n",
       "  '##bl',\n",
       "  '##og',\n",
       "  'and',\n",
       "  'the',\n",
       "  'number',\n",
       "  'of',\n",
       "  'the',\n",
       "  'invited',\n",
       "  'users',\n",
       "  'is',\n",
       "  'greater',\n",
       "  'than',\n",
       "  'or',\n",
       "  'equal',\n",
       "  'to',\n",
       "  'each',\n",
       "  'client',\n",
       "  'upon',\n",
       "  'receiving',\n",
       "  'the',\n",
       "  'invitation',\n",
       "  'information',\n",
       "  'creating',\n",
       "  'an',\n",
       "  'invitation',\n",
       "  'information',\n",
       "  'guide',\n",
       "  'to',\n",
       "  'guide',\n",
       "  'the',\n",
       "  'users',\n",
       "  'who',\n",
       "  'have',\n",
       "  'not',\n",
       "  'registered',\n",
       "  'the',\n",
       "  'micro',\n",
       "  '##bl',\n",
       "  '##og',\n",
       "  'to',\n",
       "  'register',\n",
       "  'the',\n",
       "  'micro',\n",
       "  '##bl',\n",
       "  '##og',\n",
       "  'the',\n",
       "  'method',\n",
       "  'further',\n",
       "  'comprises',\n",
       "  'when',\n",
       "  'pre',\n",
       "  '##de',\n",
       "  '##ter',\n",
       "  '##mined',\n",
       "  'time',\n",
       "  'is',\n",
       "  'reached',\n",
       "  'server',\n",
       "  'actively',\n",
       "  'sending',\n",
       "  'invitation',\n",
       "  'information',\n",
       "  'to',\n",
       "  'at',\n",
       "  'least',\n",
       "  'one',\n",
       "  'client',\n",
       "  'corresponding',\n",
       "  'to',\n",
       "  'at',\n",
       "  'least',\n",
       "  'one',\n",
       "  'user',\n",
       "  'who',\n",
       "  'has',\n",
       "  'not',\n",
       "  'registered',\n",
       "  'the',\n",
       "  'micro',\n",
       "  '##bl',\n",
       "  '##og',\n",
       "  '[SEP]'],\n",
       " array([1., 0.], dtype=float32),\n",
       " 134]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_len_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data_with_len_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([13, 23, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n",
       "        41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57,\n",
       "        58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74,\n",
       "        75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91,\n",
       "        92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106,\n",
       "        107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119,\n",
       "        120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132,\n",
       "        133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145,\n",
       "        146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158,\n",
       "        159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171,\n",
       "        172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184,\n",
       "        185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
       "        198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 209, 210, 211,\n",
       "        212, 213, 214, 215, 216, 217, 219, 220, 221, 223, 224, 225, 226,\n",
       "        229, 230, 231, 232, 234, 236, 237, 240, 242, 243, 246, 247, 248,\n",
       "        249, 252, 253, 259, 262, 264, 266, 268, 269, 271, 284, 294, 324,\n",
       "        456], dtype=object),\n",
       " array([ 1,  2,  2,  2,  2,  2,  5,  1,  3,  3,  5,  5,  5,  4,  6,  3,  8,\n",
       "         6,  5, 10,  5, 10,  5, 14,  7,  9, 10,  9,  2,  9, 13,  8,  8,  7,\n",
       "        13, 14, 12, 10,  8, 11, 17, 18, 20, 20, 24, 18, 15, 18, 15, 11, 17,\n",
       "        23, 26, 18, 13, 26, 31, 27, 25, 29, 24, 25, 31, 24, 32, 15, 31, 18,\n",
       "        25, 34, 40, 33, 30, 26, 28, 31, 25, 31, 32, 18, 29, 30, 32, 33, 34,\n",
       "        30, 38, 31, 32, 32, 31, 31, 25, 46, 34, 20, 41, 26, 32, 34, 33, 38,\n",
       "        39, 41, 27, 23, 26, 32, 40, 39, 32, 35, 31, 30, 44, 39, 36, 46, 47,\n",
       "        39, 37, 42, 39, 44, 34, 42, 47, 38, 43, 46, 43, 31, 42, 39, 36, 33,\n",
       "        34, 33, 32, 37, 32, 32, 30, 26, 26, 30, 24, 22, 20, 19, 17, 16, 23,\n",
       "        19, 15, 13, 17,  8,  8, 10, 12,  8,  6,  6,  7, 10, 10,  5,  9,  4,\n",
       "         5,  5,  9,  3,  4,  6,  9,  9,  7,  5,  1,  4,  2,  4,  6,  2,  2,\n",
       "         3,  3,  1,  3,  3,  3,  2,  3,  3,  2,  1,  2,  3,  5,  2,  2,  2,\n",
       "         1,  2,  2,  2,  4,  2,  1,  1,  2,  2,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(data_with_len_train)[:, 2], return_counts = True)\n",
    "# majority are in 512 and above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_batch(data_with_len, BATCH_SIZE):\n",
    "    data_with_len = sorted(data_with_len, key=lambda x: x[2])\n",
    "    sorted_all = [([get_ids(sent_lab[0]),\n",
    "                get_mask(sent_lab[0]),\n",
    "                get_segments(sent_lab[0])],\n",
    "               sent_lab[1])\n",
    "              for sent_lab in data_with_len] \n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
    "                                             output_types=(tf.int32, tf.int32))\n",
    "    \n",
    "\n",
    "    train_batched = train_dataset.padded_batch(BATCH_SIZE, # this is the pre-processed input to feed into BERT embedding layer and DCNN model\n",
    "                                       padded_shapes=((3, 512), [2]), # since our label is one-hot encoding format, need to use [2] rather than ()\n",
    "                                       padding_values=(0, 0))\n",
    "    return train_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the batches for train set:\n",
    "train_batched = to_batch(data_with_len_train, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 3, 512), dtype=int32, numpy=\n",
       " array([[[  101, 17261, 10099, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101, 16175,  5162, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2422, 12495, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  101, 17782, 21335, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2835,  3295, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101, 28406,  1997, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(32, 2), dtype=int32, numpy=\n",
       " array([[1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0]], dtype=int32)>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_batched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the batches for evaluation set:\n",
    "test_batched = to_batch(data_with_len_test, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 3, 512), dtype=int32, numpy=\n",
       " array([[[  101, 19160,  8332, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  9265,  2005, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  4725,  2005, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  101, 22160,  2102, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2224,  1997, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  5080,  1998, ...,     0,     0,     0],\n",
       "         [    1,     1,     1, ...,     0,     0,     0],\n",
       "         [    0,     0,     0, ...,     0,     0,     0]]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(32, 2), dtype=int32, numpy=\n",
       " array([[0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1]], dtype=int32)>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_batched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Count:1000, Positive Label Count:[595 405], Positive Class Ratio:[0.595 0.405], Negative Class Ratio = [0.405 0.595]\n"
     ]
    }
   ],
   "source": [
    "# double check the label distribution\n",
    "# Validation set\n",
    "count = 0\n",
    "sum_positive = 0\n",
    "for element in test_batched:\n",
    "    count += len(element[1])\n",
    "    sum_positive += sum(element[1])\n",
    "\n",
    "print(f'Sample Count:{count}, Positive Label Count:{sum_positive}, Positive Class Ratio:{sum_positive/count}, Negative Class Ratio = {1-sum_positive/count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Count:4000, Positive Label Count:[2397 1603], Positive Class Ratio:[0.59925 0.40075], Negative Class Ratio = [0.40075 0.59925]\n"
     ]
    }
   ],
   "source": [
    "# double check the label distribution\n",
    "# Train set\n",
    "count = 0\n",
    "sum_positive = 0\n",
    "for element in train_batched:\n",
    "    count += len(element[1])\n",
    "    sum_positive += sum(element[1])\n",
    "\n",
    "print(f'Sample Count:{count}, Positive Label Count:{sum_positive}, Positive Class Ratio:{sum_positive/count}, Negative Class Ratio = {1-sum_positive/count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNNBERTEmbedding(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_filters=50,\n",
    "                 FFN_units=512,\n",
    "                 nb_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 name=\"dcnn\"):\n",
    "        super(DCNNBERTEmbedding, self).__init__(name=name)\n",
    "        \n",
    "        self.bert_layer = hub.KerasLayer(\n",
    "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "            trainable=False)\n",
    "\n",
    "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
    "                                    kernel_size=5,\n",
    "                                    padding=\"valid\",\n",
    "                                    activation=\"relu\")\n",
    "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
    "                                     kernel_size=50,\n",
    "                                     padding=\"valid\",\n",
    "                                     activation=\"relu\")\n",
    "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
    "                                      kernel_size=100,\n",
    "                                      padding=\"valid\",\n",
    "                                      activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if nb_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=2,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=nb_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def embed_with_bert(self, all_tokens):\n",
    "        _, embs = self.bert_layer([all_tokens[:, 0, :],\n",
    "                                   all_tokens[:, 1, :],\n",
    "                                   all_tokens[:, 2, :]])\n",
    "        return embs\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.embed_with_bert(inputs)\n",
    "\n",
    "        x_1 = self.bigram(x)\n",
    "        x_1 = self.pool(x_1)\n",
    "        x_2 = self.trigram(x)\n",
    "        x_2 = self.pool(x_2)\n",
    "        x_3 = self.fourgram(x)\n",
    "        x_3 = self.pool(x_3)\n",
    "        \n",
    "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_FILTERS = 100\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = 2\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dcnn = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n",
    "                         FFN_units=FFN_UNITS,\n",
    "                         nb_classes=NB_CLASSES,\n",
    "                         dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NB_CLASSES == 2:\n",
    "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "else:\n",
    "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"ckpt_BERT_CNN_US_5years/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ckpt_manager.save()\n",
    "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "    125/Unknown - 4415s 35s/step - loss: 0.9421 - accuracy: 0.5767Checkpoint saved at ckpt_BERT_CNN_US_5years/.\n",
      "125/125 [==============================] - 5422s 43s/step - loss: 0.9421 - accuracy: 0.5767 - val_loss: 0.6834 - val_accuracy: 0.5910\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6556 - accuracy: 0.6258 Checkpoint saved at ckpt_BERT_CNN_US_5years/.\n",
      "125/125 [==============================] - 5477s 44s/step - loss: 0.6556 - accuracy: 0.6258 - val_loss: 0.6561 - val_accuracy: 0.6080\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.5957 - accuracy: 0.6862 Checkpoint saved at ckpt_BERT_CNN_US_5years/.\n",
      "125/125 [==============================] - 5456s 44s/step - loss: 0.5957 - accuracy: 0.6862 - val_loss: 0.7070 - val_accuracy: 0.5650\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.4978 - accuracy: 0.7595 Checkpoint saved at ckpt_BERT_CNN_US_5years/.\n",
      "125/125 [==============================] - 5893s 47s/step - loss: 0.4978 - accuracy: 0.7595 - val_loss: 0.7785 - val_accuracy: 0.5790\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.4376 - accuracy: 0.8065 Checkpoint saved at ckpt_BERT_CNN_US_5years/.\n",
      "125/125 [==============================] - 4174s 33s/step - loss: 0.4376 - accuracy: 0.8065 - val_loss: 0.7488 - val_accuracy: 0.6030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f11520908d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting time\n",
    "Dcnn.fit(train_batched,\n",
    "         validation_data = (test_batched),\n",
    "         epochs=NB_EPOCHS,\n",
    "         callbacks=[MyCustomCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify from original to_batch in order to not sort to different order because for ensemble we need to preserve the same order among all models\n",
    "def prediction_to_batch(data_with_len, BATCH_SIZE):\n",
    "    # data_with_len = sorted(data_with_len, key=lambda x: x[2])\n",
    "    sorted_all = [([get_ids(sent_lab[0]),\n",
    "                get_mask(sent_lab[0]),\n",
    "                get_segments(sent_lab[0])],\n",
    "               sent_lab[1])\n",
    "              for sent_lab in data_with_len] \n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
    "                                             output_types=(tf.int32, tf.int32))\n",
    "    \n",
    "\n",
    "    train_batched = train_dataset.padded_batch(BATCH_SIZE, # this is the pre-processed input to feed into BERT embedding layer and DCNN model\n",
    "                                       padded_shapes=((3, 512), [2]), # make from None to 512 fixed dimension\n",
    "                                       padding_values=(0, 0)) # drop_remainder = True (optional)\n",
    "    return train_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the batches for evaluation set:\n",
    "test_batched_prediction = prediction_to_batch(data_with_len_test, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84799695, 0.13072672],\n",
       "       [0.33215982, 0.66788644],\n",
       "       [0.23617092, 0.7885741 ],\n",
       "       ...,\n",
       "       [0.19518188, 0.7800475 ],\n",
       "       [0.5346638 , 0.48711532],\n",
       "       [0.8942262 , 0.10214731]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test = Dcnn.predict(test_batched_prediction)\n",
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Predict_Output/BERT_CNN_5yr_abstract_title_dev_prob.csv', pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = [np.argmax(pred) for pred in \n",
    "             pred_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_binary = df['quality_rank'][4000:].values\n",
    "y_test_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.603\n",
      "Auc: 0.534\n",
      "Detail:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.90      0.73       595\n",
      "           1       0.53      0.17      0.26       405\n",
      "\n",
      "    accuracy                           0.60      1000\n",
      "   macro avg       0.57      0.53      0.49      1000\n",
      "weighted avg       0.58      0.60      0.54      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Accuracy, Precision, Recall\n",
    "accuracy = metrics.accuracy_score(y_test_binary, predicted)\n",
    "auc = metrics.roc_auc_score(y_test_binary, predicted)  # predicted_prob), check doc, seems the second argument required to be shape (n_samples,) for binary case \n",
    "                            #multi_class=\"ovr\") # check documentation and seems \"ovr\" not good for only binary target class\n",
    "print(\"Accuracy:\",  round(accuracy,3))\n",
    "print(\"Auc:\", round(auc,3))\n",
    "print(\"Detail:\")\n",
    "print(metrics.classification_report(y_test_binary, predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
